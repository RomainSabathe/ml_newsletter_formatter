<!DOCTYPE html>
<html>
<title>ML newsletter</title>
<meta charset="UTF-8">

<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<!-- Optional theme -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Roboto'>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<style>
	html,body,h1,h2,h3,h4,h5,h6 {
        font-family: "Roboto", sans-serif;
    }

    p {
        font-size: 15px;
    }

	.category{
	  padding: 20px;
	  margin: auto;
	  background: #F5F5F5;
	  margin-bottom: 25px;
	  box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
	  transition: all 0.3s; 
	  border-radius: 3px;
	}

	.intro-text{
	  padding: 20px;
	  margin: auto;
	  margin-bottom: 10px;
	  background: #F0EEEE;
	  box-shadow: 0 6px 12px rgba(0,0,0,0.16), 0 6px 12px rgba(0,0,0,0.23);
	  transition: all 0.3s; 
	}

    .category-title{
      margin-bottom: 30px;
    }

	.category-logo{
	  margin-right: 10px;
	}

	.article-logo{
	  margin-right: 3px;
	}

    .recommended{
	  margin-right: 2px;
    }

    .quote{
      font-family: "Times New Roman", serif;
      font-size: 15px;
    }
</style>


<body>

<!-- Page Container -->
<div class="container-fluid">

  
  <!-- The Grid -->
  <div class="row">
  
      <div class="col-md-10 col-md-offset-1">

          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/653/653500.png"
                       class="fa fa-fw category-logo">General Machine Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://neurips.cc/Conferences/2019/AcceptedPapersInitial">NeurIPS 2019: list of accepted papers</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    
                  </h6>

                  
                                    <p>The title says it all. Notice that for now we only have access to the title of the papers and not their content.</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/149/149243.png"
                       class="fa fa-fw category-logo">Deep Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1904.02323v3">[IEEE VAST 2020] Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Georgia Tech
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://arxiv.org/abs/1904.02323v3">
                          <img src=https://i.ibb.co/7XLG7dz/Selection-966.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <blockquote class="blockquote">
                     <p class="quote">[..] Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. <b>We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier's learned representations and informs future neural network architecture design.</b> The Summit visualization runs in modern web browsers and is open-sourced. </p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/25/25231.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1908.11161v2">[arXiv] InferPy: Probabilistic Modeling with Deep Neural Networks Made Easy</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    University of Almeria, IDSIA
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://arxiv.org/abs/1908.11161v2">
                          <img src=https://i.ibb.co/VYRknTq/Selection-967.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <blockquote class="blockquote">
                     <p class="quote">InferPy is a Python package for probabilistic modeling with deep neural networks. InferPy defines a user-friendly API which trades-off model complexity with ease of use, unlike other libraries whose focus is on dealing with very general probabilistic models at the cost of having a more complex API. In particular, Inferpy allows to define, learn and evaluate general hierarchical probabilistic models containing deep neural networks in a compact and simple way. InferPy is built on top of Tensorflow, Edward2 and Keras. "</p>
                  </blockquote>
                                    <p>See also the <a href=https://github.com/PGM-Lab/InferPy/>GitHub link</a>.</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/179/179121.png"
                       class="fa fa-fw category-logo">Optimisation & Learning Theory
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1905.09997">[NeurIPS 2019] Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Mila, Universite de Montreal, University of British Columbia
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://arxiv.org/abs/1905.09997">
                          <img src=https://i.ibb.co/Yp6mJws/Selection-964.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <blockquote class="blockquote">
                     <p class="quote">Recent works have shown that stochastic gradient descent (SGD) achieves the fast convergence rates of full-batch gradient descent for over-parameterized models satisfying certain interpolation conditions. However, the step-size used in these works depends on unknown quantities, and SGD's practical performance heavily relies on the choice of the step-size. We propose to use line-search methods to automatically set the step-size when training models that can interpolate the data. We prove that SGD with the classic Armijo line-search attains the fast convergence rates of full-batch gradient descent in convex and strongly-convex settings. We also show that under additional assumptions, SGD with a modified line-search can attain a fast rate of convergence for non-convex functions. Furthermore, we show that a stochastic extra-gradient method with a Lipschitz line-search attains a fast convergence rate for an important class of non-convex functions and saddle-point problems satisfying interpolation. We then give heuristics to use larger step-sizes and acceleration with our line-search techniques. We compare the proposed algorithms against numerous optimization methods for standard classification tasks using both kernel methods and deep networks. The proposed methods are robust and result in competitive performance across all models and datasets. Moreover, for the deep network models, SGD with our line-search results in both faster convergence and better generalization.</p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1906.00904">[NeurIPS 2019] Deep ReLU Networks Have Surprisingly Few Activation Patterns</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Facebook AI Research, University of Pennsylvania
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://arxiv.org/abs/1906.00904">
                          <img src=https://i.ibb.co/Cty3vJk/Selection-965.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <blockquote class="blockquote">
                     <p class="quote">The success of deep networks has been attributed in part to their expressivity: per parameter, deep networks can approximate a richer class of functions than shallow networks. In ReLU networks, the number of activation patterns is one measure of expressivity; and the maximum number of patterns grows exponentially with the depth. However, recent work has showed that the practical expressivity of deep networks - the functions they can learn rather than express - is often far from the theoretical maximum. In this paper, we show that the average number of activation patterns for ReLU networks at initialization is bounded by the total number of neurons raised to the input dimension. We show empirically that this bound, which is independent of the depth, is tight both at initialization and during training, even on memorization tasks that should maximize the number of activation patterns. <b>Our work suggests that realizing the full expressivity of deep networks may not be possible in practice, at least with current methods.</b> </p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

                          
          </div>
                </div>

  <!-- End Grid -->
  </div>

    
  <!-- End Page Container -->
</div>

</body>
</html>