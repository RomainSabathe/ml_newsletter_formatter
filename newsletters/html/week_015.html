<!DOCTYPE html>
<html>
<title>ML newsletter</title>
<meta charset="UTF-8">

<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<!-- Optional theme -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Roboto'>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<style>
	html,body,h1,h2,h3,h4,h5,h6 {font-family: "Roboto", sans-serif}

	.category{
	  padding: 20px;
	  margin: auto;
	  background: #F5F5F5;
	  margin-bottom: 25px;
	  box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
	  transition: all 0.3s; 
	  border-radius: 3px;
	}

    .category-title{
      margin-bottom: 30px;
    }

	.category-logo{
	  margin-right: 10px;
	}

	.article-logo{
	  margin-right: 3px;
	}

    .recommended{
	  margin-right: 2px;
    }

    .quote{
      font-size: 11px;
    }
</style>


<body>

<!-- Page Container -->
<div class="container" style="max-width:1400px;">

  <!-- The Grid -->
  <div class="row">
  
      <!-- Dummy container to force centering. Yes. I'm a front-end noob. -->
      <div class="col-sm-2"></div>

      <div class="col-sm-8">

          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/653/653500.png"
                       class="fa fa-fw category-logo">General Machine Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="http://jackterwilliger.com/biological-neural-networks-part-i-spiking-neurons/">(A bit of) Biological Neural Networks - Part I, Spiking Neurons</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                        <span class="badge">
                                  Recommended
                              </span>
                                                    Jack Terwilliger
                  </h6>
                  <p>A long post of great quality with visualisations and equations. The most famous spiking models are introduced, as well as an introduction on dynamical systems and the different types of bifurcations. There is a Lot of information so it's hard to sink it all in but it serves as a great introduction overall.</p>
                                </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1804.04849">The unreasonable effectiveness of the forget gate</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    University of Cambridge
                  </h6>
                  <p></p>
                                    <blockquote class="blockquote">
                     <p class="quote">Surprisingly, the results indicate that the JANET yields higher accuracies than the standard, LSTM. Moreover, JANET is among the top performing models on all of the analysed datasets. Thus, by simplifying the LSTM, we not only save on computational cost but also gain in test set accuracy!</p>
                  </blockquote>
                                </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://eng.uber.com/vine/">VINE: An Open Source Interactive Data Visualization Tool for Neuroevolution</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Uber
                  </h6>
                  <p>Uber open source the monitoring tool they used to solve the Humanoid Locomotion task with genetic algorithms.</p>
                                </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/149/149243.png"
                       class="fa fa-fw category-logo">Deep Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1804.06094">Sparse Unsupervised Capsules Generalize Better</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Incubator 491
                  </h6>
                  <p></p>
                                    <blockquote class="blockquote">
                     <p class="quote">We show that unsupervised training of latent capsule layers using only the reconstruction loss, without masking to select the correct output class, causes a loss of equivariances and other desirable capsule qualities. This implies that supervised capsules networks can't be very deep. Unsupervised sparsening of latent capsule layer activity both restores these qualities and appears to generalize better than supervised masking, while potentially enabling deeper capsules networks.</p>
                  </blockquote>
                                </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/722/722174.png"
                       class="fa fa-fw category-logo">Computer Vision
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1804.05298">Semantic Feature Augmentation in Few-shot Learning</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Fudan University, Princeton University, University of British Columbia
                  </h6>
                  <p></p>
                                    <blockquote class="blockquote">
                     <p class="quote">A fundamental problem with few-shot learning is the scarcity of data in training. A natural solution to alleviate this scarcity is to augment the existing images for each training class. However, directly augmenting samples in image space may not necessarily, nor sufficiently, explore the intra-class variation. To this end, we propose to directly synthesize instance features by leveraging the semantics of each class. Essentially, a novel auto-encoder network dual TriNet, is proposed for feature augmentation.</p>
                  </blockquote>
                                </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1804.06516">Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    NVIDIA, University of Toronto
                  </h6>
                  <p></p>
                                    <blockquote class="blockquote">
                     <p class="quote">We present a system for training deep neural networks for object detection using synthetic images. To handle the variability in real-world data, the system relies upon the technique of domain randomization, in which the parameters of the simulator−such as lighting, pose, object textures, etc.−are randomized in non-realistic ways to force the neural network to learn the essential features of the object of interest.</p>
                  </blockquote>
                                </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/179/179121.png"
                       class="fa fa-fw category-logo">Optimisation & Learning Theory
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1804.04235">Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Google Brain
                  </h6>
                  <p>A new optimiser that claims to be as efficient as Adam while requiring little memory overhead (compared to Adam that stores the second moment estimators for every single parameter in the network).</p>
                                </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1804.05862">Compressibility and Generalization in Large-Scale Deep Learning</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    
                  </h6>
                  <p></p>
                                    <blockquote class="blockquote">
                     <p class="quote">Modern neural networks are highly overparameterized, with capacity to substantially overfit to training data. Nevertheless, these networks often generalize well in practice. It has also been observed that trained networks can often be 'compressed' to much smaller representations. The purpose of this paper is to connect these two empirical observations. Our main technical result is a generalization bound for compressed networks based on the compressed size.</p>
                  </blockquote>
                                </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/112/112272.png"
                       class="fa fa-fw category-logo">Gans & Adversarial Attacks
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <img src="https://image.flaticon.com/icons/png/512/25/25231.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1804.04732">MUNIT: Multimodal UNsupervised Image-to-image Translation</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Cornell University, NVIDIA
                  </h6>
                  <p>Given an image of a specific modality (say a dog with a certain pose), you want to generate a similar content with new modalities (e..g a cat with the same pose. Or a lion with the same pose, etc.). This paper explores this idea. Since it's unsupervised, I see this as a sort of CycleGAN. Although here admittedly the novelty of the paper is that you can transfer an image to a number of different modalities (and not just one). Code and images <a href=https://github.com/NVlabs/MUNIT>here</a>. </p>
                                </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/249/249156.png"
                       class="fa fa-fw category-logo">Reinforcement Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/174/174858.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://medium.com/applied-data-science/how-to-build-your-own-world-model-using-python-and-keras-64fb388ba459">Hallucinogenic Deep Reinforcement Learning Using Python and Keras</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                        <span class="badge">
                                  Recommended
                              </span>
                                                    David Foster
                  </h6>
                  <p>A few weeks ago we mentioned in this newsletter <a href=https://arxiv.org/abs/1803.10122>a new paper from Ha & Schmidhuber</a> called 'World Models' where an agent was building a sort of mental representation of its environment. It wasn't very clear to me was all this was about. Well, this blog post is an easy and simplified explanation of the paper. It also provides an implementation.</p>
                                </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://blog.openai.com/evolved-policy-gradients/">Evolved Policy Gradients</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    OpenAI
                  </h6>
                  <p>Very interesting work. They present an algorithm called Evolved Policy Gradients which aim aim at improving the generalisation capabilities of agents. The core idea is to dynamically shape the reward function via an evolutionary algorithm. The whole process works in two steps: at a lower level, the agent tries to solve a specific task (like moving the arm to a given location) via SGD and at a higher level, the evolutionary algorithm modifies the reward function to lead to higher returns.</p>
                                </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/25/25231.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://github.com/cshenton/neuroevolution">Neuroevolution for Reinforcement Learning</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Charles Shenton
                  </h6>
                  <p>An alternative implementation of the Neuroevoluation paper from Uber. Uses Tensorflow and Gym.</p>
                                </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/484/484582.png"
                       class="fa fa-fw category-logo">Natural Language Processing
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="http://ruder.io/text-classification-tensorflow-estimators/">Text Classification with Tensorflow Estimators</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Sebastian Ruder
                  </h6>
                  <p>A tutorial that introduces some of the recent Tensorflow high-level APIs used to create a model quickly. The post is the 4th of a series so some of the functions and classes that are used feel sometimes mysterious. Nonetheless, this post can serve as an introduction to NLP and can point you to some of those new ways of writing models with Tensorflow.</p>
                                </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://imageog.flaticon.com/icons/png/512/248/248114.png"
                       class="fa fa-fw category-logo">Production & Engineering
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/174/174858.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765">How to unit test machine learning code</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                        <span class="badge">
                                  Recommended
                              </span>
                                                    Chase Roberts
                  </h6>
                  <p>Unit testing models has been a dream of mine for a while... Well, this article is not ground breaking per say, but it at least shows that you can thoroughly test some of the properties of your model. Among the examples presented: is the loss different from zero? Do the tensors have the expected shape? Do all the parameters get updated? Do they get updated when they are supposed to? Oh, and can I say that some of the problems encountered by the author come from weird designs from Tensorflow, like setting the training of batch norm to false by default? :smirk:</p>
                                </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/263/263051.png"
                       class="fa fa-fw category-logo">Everything Else
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="http://www.argmin.net/2018/04/16/ethical-rewards/">The Ethics of Reward Shaping</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Ben Recht for 'arg min' blog
                  </h6>
                  <p></p>
                                </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/174/174858.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7">Artificial Intelligence - The Revolution Hasn't Happened Yet</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Michael Jordan
                  </h6>
                  <p>A long essay which has been quite debated. My limited understanding of it would be: 'making a human-like AI is not going to be achieved in the near future. In the meantime, there are other challenges to tackle like how to structure AI as a 'human augmentation' system in such a way that is safe and stable. For instance: how to design a network of self-driving cars?'. He advocates for a new field that he described as the AI equivalent of civil engineering: civil engineers are building bridges and buildings that are safe and stable. Can we do the same?</p>
                                </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://fosterelli.co/executing-gradient-descent-on-the-earth">Executing gradient descent on earth</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Chris Foster
                  </h6>
                  <p>Funny thing to do. Although I didn't really understand the equation he used to compute the slopes. Is it just me?</p>
                                </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://www.quantamagazine.org/machine-learnings-amazing-ability-to-predict-chaos-20180418/">Machine Learning's 'Amazing' Ability to Predict Chaos</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    University of Maryland
                  </h6>
                  <p>Some researchers used reservoir networks (yes) to predict the evolution of a chaotic system over time (the dynamic of a flame for instance) and observed quite astonishing predicting performance. Quote: 'As a ballpark estimate, you’d have to measure a typical system’s initial conditions 100,000,000 times more accurately to predict its future evolution [as well as the network did].'</p>
                                </div>
              <hr>

                          
          </div>
                </div>
      
  <!-- Dummy container to force centering. Yes. I'm a front-end noob. -->
  <div class="col-sm-2"></div>

  <!-- End Grid -->
  </div>
  
  <!-- End Page Container -->
</div>

</body>
</html>