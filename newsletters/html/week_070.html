<!DOCTYPE html>
<html>
<title>ML newsletter</title>
<meta charset="UTF-8">

<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<!-- Optional theme -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Roboto'>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<style>
	html,body,h1,h2,h3,h4,h5,h6 {
        font-family: "Roboto", sans-serif;
    }

    p {
        font-size: 15px;
    }

	.category{
	  padding: 20px;
	  margin: auto;
	  background: #F5F5F5;
	  margin-bottom: 25px;
	  box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
	  transition: all 0.3s; 
	  border-radius: 3px;
	}

	.intro-text{
	  padding: 20px;
	  margin: auto;
	  margin-bottom: 10px;
	  background: #F0EEEE;
	  box-shadow: 0 6px 12px rgba(0,0,0,0.16), 0 6px 12px rgba(0,0,0,0.23);
	  transition: all 0.3s; 
	}

    .category-title{
      margin-bottom: 30px;
    }

	.category-logo{
	  margin-right: 10px;
	}

	.article-logo{
	  margin-right: 3px;
	}

    .recommended{
	  margin-right: 2px;
    }

    .quote{
      font-family: "Times New Roman", serif;
      font-size: 15px;
    }
</style>


<body>

<!-- Page Container -->
<div class="container-fluid">

  
  <!-- The Grid -->
  <div class="row">
  
      <div class="col-md-10 col-md-offset-1">

          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/149/149243.png"
                       class="fa fa-fw category-logo">Deep Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://myrtle.ai/how-to-train-your-resnet-8-bag-of-tricks/">How to Train Your ResNet 8: Bag of Tricks</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Myrtle
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://myrtle.ai/how-to-train-your-resnet-8-bag-of-tricks/">
                          <img src=https://i.ibb.co/PNBktdN/Selection-959.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <p>If you believe in the idea of <a href=https://hbr.org/2015/10/how-1-performance-improvements-led-to-olympic-gold>marginal improvements</a> then you will very likely like this post. It is actually the last in a <a href=https://myrtle.ai/how-to-train-your-resnet/>series</a> of posts that covers efficient training on CIFAR-10 by reviewing hyperparameter optimization, architecture, regularisation and all the rest you could think of. Here the author uses a palette of tricks that has usually been applied to ImageNet and distilled among several papers. The metric for this benchmark is the time to reach 94% accuracy on the validation set. Regardless of how questionable this metric might be, the list of tricks is interesting and probably worth taking note of. More importantly, the last experiment shows that by optimising this metric, one can actually improve overall accuracy if we let the training run for longer. In other words, optimising for a reasonable accuracy very quickly seems to be a rewarding strategy if we are interested in obtaining a 'good' accuracy (regardless of training time).</p>
              </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1908.07086v1">[ICCV 2019] Human uncertainty makes classification more robust</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Princeton University
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://arxiv.org/abs/1908.07086v1">
                          <img src=https://i.ibb.co/qdtWk36/Selection-956.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <blockquote class="blockquote">
                     <p class="quote">The classification performance of deep neural networks has begun to asymptote at near-perfect levels. However, their ability to generalize outside the training set and their robustness to adversarial attacks have not. In this paper, we make progress on this problem by training with full label distributions that reflect human perceptual uncertainty. We first present a new benchmark dataset which we call CIFAR10H, containing a full distribution of human labels for each image of the CIFAR10 test set. We then show that, while contemporary classifiers fail to exhibit human-like uncertainty on their own, explicit training on our dataset closes this gap, supports improved generalization to increasingly out-of-training-distribution test datasets, and confers robustness to adversarial attacks.</p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/722/722174.png"
                       class="fa fa-fw category-logo">Computer Vision
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1908.07191v1">[ICCV 2019] Make a Face: Towards Arbitrary High Fidelity Face Manipulation</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    The Chinese University of Hong Kong, Tsinghua University, University of Electronic Science and Technology of China, NLPR, CASIA, SenseTime Research
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://arxiv.org/abs/1908.07191v1">
                          <img src=https://i.ibb.co/jWYcnrG/Selection-955.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <blockquote class="blockquote">
                     <p class="quote">[...] In this work, we propose Additive Focal Variational Auto-encoder (AF-VAE), a novel approach that can arbitrarily manipulate high-resolution face images using a simple yet effective model and only weak supervision of reconstruction and KL divergence losses. First, a novel additive Gaussian Mixture assumption is introduced with an unsupervised clustering mechanism in the structural latent space, which endows better disentanglement and boosts multi-modal representation with external memory. Second, to improve the perceptual quality of synthesized results, two simple strategies in architecture design are further tailored and discussed on the behavior of Human Visual System (HVS) for the first time, allowing for fine control over the model complexity and sample quality. [...]</p>
                  </blockquote>
                                    <p>I would highly recommend that you open the pdf of the paper and zoom in to see the details. The results shown are very impressive.</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/179/179121.png"
                       class="fa fa-fw category-logo">Optimisation & Learning Theory
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://medium.com/autonomous-learning-library/radam-a-new-state-of-the-art-optimizer-for-rl-442c1e830564">RAdam: A New State-of-the-Art Optimizer for RL?</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Chris Nota
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://medium.com/autonomous-learning-library/radam-a-new-state-of-the-art-optimizer-for-rl-442c1e830564">
                          <img src=https://miro.medium.com/max/1568/1*V4HtlKqk5OJfM3wi7cMdhg.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <p>Last week, we mentioned <a href=https://arxiv.org/abs/1908.03265>On the Variance of the Adaptative Learning Rate and Beyond</a>. This paper got quite a bit of attention. Authors introduce RAdam, an optimizer based on Adam that aims at solving the learning rate warmup 'problem'. In other words: theoretically you could use RAdam and obtain close to the same results as if you were using Adam and a learning rate warmup scheme. This blog post compares Adam and RAdam in the setting of reinforcement learning, on Atari games.The author concludes that RAdam is not the breakthrough that will change reinforcement learning forever. However I note that on the 6 experiments he tried, RAdam was in most cases equivalent to Adam (or very slightly better) and marginally better on a few remaining cases. This would tend to suggest that RAdam is at least not worse than Adam (with the configuration the author chose, on this specific task, yada yada). So it's still worth a try.</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/484/484582.png"
                       class="fa fa-fw category-logo">Natural Language Processing
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1908.05620v1">[EMNLP 2019] Visualizing and Understanding the Effectiveness of BERT</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Beihang University, Microsoft Research
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://arxiv.org/abs/1908.05620v1">
                          <img src=https://i.ibb.co/w4cDKHk/Selection-957.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <blockquote class="blockquote">
                     <p class="quote">Language model pre-training, such as BERT, has achieved remarkable results in many NLP tasks. However, it is unclear why the pre-training-then-fine-tuning paradigm can improve performance and generalization capability across different tasks. In this paper, we propose to visualize loss landscapes and optimization trajectories of fine-tuning BERT on specific datasets. First, we find that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier optimization compared with training from scratch. We also demonstrate that the fine-tuning procedure is robust to overfitting, even though BERT is highly over-parameterized for downstream tasks. Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of BERT are more invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language.</p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/680/680208.png"
                       class="fa fa-fw category-logo">Data Science & Visualisations
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/25/25231.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://github.com/uwdata/visualization-curriculum">A data visualization curriculum of interactive notebooks</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    UW Interactive Data Lab
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://github.com/uwdata/visualization-curriculum">
                          <img src=https://i.ibb.co/9ZpwGxX/Selection-958.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <blockquote class="blockquote">
                     <p class="quote">A data visualization curriculum of interactive notebooks, using Vega-Lite and Altair. This repository contains a series of Python-based Jupyter notebooks, a corresponding set of JavaScript notebooks are available online on Observable.</p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://imageog.flaticon.com/icons/png/512/248/248114.png"
                       class="fa fa-fw category-logo">Production & Engineering
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/25/25231.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://www.snorkel.org/hello-world-v-0-9/">Introducing the New Snorkel · Snorkel</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Snorkel
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://www.snorkel.org/hello-world-v-0-9/">
                          <img src=https://www.snorkel.org/doks-theme/assets/images/layout/Overview.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <p>Here is a package I had never heard of but that seems fairly popular, shame on me. It appears that Snorkel is a tool for <i>programmatically building and managing training datasets</i>. Originally it was designed for text-based datasets, but this blog post introduces its newest release where authors announce that Snorkel now supports other modalities (including numpy arrays, pandas dataframes,...). Exciting! I'll keep an eye on that one.</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/263/263051.png"
                       class="fa fa-fw category-logo">Everything Else
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://thegradient.pub/introducing-retrospectives/">Introducing Retrospectives: 'Real Talk' for your Past Papers</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Ryan Lowe - Mila, Facebook AI, Standford University, Uber AI
                  </h6>

                  
                                    <blockquote class="blockquote">
                     <p class="quote">Nowadays, part of reading papers is trying to decipher which claims are technically sound. It’s not uncommon to read a paper in machine learning and think: 'Okay, what are they trying to hide? What secret trick do you need to actually get this to work?' For many researchers, this skepticism has been hard-earned trying to build on top of cool ideas that simply didn’t work as advertised. What’s amazing is how often this happens without us thinking twice about it.</p>
                  </blockquote>
                                    <p>This post introduces <a href=https://ml-retrospectives.github.io/>ML Retrospectives</a> and its NeurIPS workshop. The idea is simple: the author of a paper might have a different view of their own work at the time they are trying to get the paper accepted compared to years later when they describe them to colleagues. They might have a different opinion; they might be aware of new limitations; they might have not mentioned failed attempts. ML Retrospectives is a place for authors to share this kind of precious information. See an <a href=https://ml-retrospectives.github.io/published_retrospectives/2019/adem/>example</a> for yourselves.</p>
              </div>
              <hr>

                          
          </div>
                </div>

  <!-- End Grid -->
  </div>

    
  <!-- End Page Container -->
</div>

</body>
</html>