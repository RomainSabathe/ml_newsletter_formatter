<!DOCTYPE html>
<html>
<title>ML newsletter</title>
<meta charset="UTF-8">

<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<!-- Optional theme -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Roboto'>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<style>
	html,body,h1,h2,h3,h4,h5,h6 {
        font-family: "Roboto", sans-serif;
    }

    p {
        font-size: 15px;
    }

	.category{
	  padding: 20px;
	  margin: auto;
	  background: #F5F5F5;
	  margin-bottom: 25px;
	  box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
	  transition: all 0.3s; 
	  border-radius: 3px;
	}

	.intro-text{
	  padding: 20px;
	  margin: auto;
	  margin-bottom: 10px;
	  background: #F0EEEE;
	  box-shadow: 0 6px 12px rgba(0,0,0,0.16), 0 6px 12px rgba(0,0,0,0.23);
	  transition: all 0.3s; 
	}

    .category-title{
      margin-bottom: 30px;
    }

	.category-logo{
	  margin-right: 10px;
	}

	.article-logo{
	  margin-right: 3px;
	}

    .recommended{
	  margin-right: 2px;
    }

    .quote{
      font-family: "Times New Roman", serif;
      font-size: 15px;
    }
</style>


<body>

<!-- Page Container -->
<div class="container-fluid">

  
  <!-- The Grid -->
  <div class="row">
  
      <div class="col-md-10 col-md-offset-1">

          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/653/653500.png"
                       class="fa fa-fw category-logo">General Machine Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://dfdazac.github.io/sinkhorn.html?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter">Approximating Wasserstein distances with PyTorch</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    <a href=https://dfdazac.github.io/>Daniel Draza</a>
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://dfdazac.github.io/sinkhorn.html?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter">
                          <img src=https://dfdazac.github.io/assets/img/sinkhorn_files/sinkhorn_13_1.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <p>This is a short introduction to the problem of optimal transport. I would recommend to read for those you have never read anything about optimal transport before. The problem and what we are trying to achieve is well-defined and examples are provided. However there are no explanations on how the Wasserstein distance is actually computed in practice. A public <a href=https://github.com/gpeyre/SinkhornAutoDiff>implementation of the Sinkhorn algorithm</a> is used in the examples.</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/149/149243.png"
                       class="fa fa-fw category-logo">Deep Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1905.08537v1">[ICML 2019] Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    University of Tsukuba, Yokohama National University, SkillUp AI, Shinshu University
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://arxiv.org/abs/1905.08537v1">
                          <img src=https://i.ibb.co/Pjbd0Dn/Selection-913.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <blockquote class="blockquote">
                     <p class="quote">High sensitivity of neural architecture search (NAS) methods against their input such as step-size (i.e., learning rate) and search space prevents practitioners from applying them out-of-the-box to their own problems, albeit its purpose is to automate a part of tuning process. Aiming at a fast, robust, and widely-applicable NAS, we develop a generic optimization framework for NAS. We turn a coupled optimization of connection weights and neural architecture into a differentiable optimization by means of stochastic relaxation. It accepts arbitrary search space (widely-applicable) and enables to employ a gradient-based simultaneous optimization of weights and architecture (fast). We propose a stochastic natural gradient method with an adaptive step-size mechanism built upon our theoretical investigation (robust). Despite its simplicity and no problem-dependent parameter tuning, our method exhibited near state-of-the-art performances with low computational budgets both on image classification and inpainting tasks.</p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/179/179121.png"
                       class="fa fa-fw category-logo">Optimisation & Learning Theory
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1806.01603v2">[ICML Workshop] Layer rotation: a surprisingly powerful indicator of generalization in deep networks?</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Université catholique de Louvain
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://arxiv.org/abs/1806.01603v2">
                          <img src=https://i.ibb.co/mT0bWvH/image.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <blockquote class="blockquote">
                     <p class="quote">Our work presents extensive empirical evidence that layer rotation, i.e. the evolution across training of the cosine distance between each layer's weight vector and its initialization, constitutes an impressively consistent indicator of generalization performance. In particular, larger cosine distances between final and initial weights of each layer consistently translate into better generalization performance of the final model. Interestingly, this relation admits a network independent optimum: training procedures during which all layers' weights reach a cosine distance of 1 from their initialization consistently outperform other configurations -by up to 30% test accuracy. Moreover, we show that layer rotations are easily monitored and controlled (helpful for hyperparameter tuning) and potentially provide a unified framework to explain the impact of learning rate tuning, weight decay, learning rate warmups and adaptive gradient methods on generalization and training speed. In an attempt to explain the surprising properties of layer rotation, we show on a 1-layer MLP trained on MNIST that layer rotation correlates with the degree to which features of intermediate layers have been trained.</p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/112/112272.png"
                       class="fa fa-fw category-logo">Gans & Adversarial Attacks
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1906.09840v2">[arXiv] Interactive Subspace Exploration on Generative Image Modelling</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    The University of Tokyo, National Taiwan University
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://arxiv.org/abs/1906.09840v2">
                          <img src=https://i.ibb.co/7KnSgxC/Selection-914.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <blockquote class="blockquote">
                     <p class="quote">Generative image modeling techniques such as GAN demonstrate highly convincing image generation result. However, user interaction is often necessary to obtain the desired results. Existing attempts add interactivity but require either tailored architectures or extra data. We present a human-in-the-optimization method that allows users to directly explore and search the latent vector space of generative image modeling. Our system provides multiple candidates by sampling the latent vector space, and the user selects the best blending weights within the subspace using multiple sliders. In addition, the user can express their intention through image editing tools. The system samples latent vectors based on inputs and presents new candidates to the user iteratively. An advantage of our formulation is that one can apply our method to arbitrary pre-trained model without developing specialized architecture or data. We demonstrate our method with various generative image modeling applications, and show superior performance in a comparative user study with prior art iGAN.</p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/249/249156.png"
                       class="fa fa-fw category-logo">Reinforcement Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4">Learning to Drive Smoothly in Minutes</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    <a href=https://araffin.github.io/>Antonin Raffin</a>
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4">
                          <img src=https://i.ibb.co/dchyrcy/Selection-915.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <p>This is cool little blog post, surely intended to those who are more familiar with reinforcement learning than I am! Antonin Raffin almost gives a list of ingredients to quickly train an RC car to drive smoothly in a simulation. He collects ~5 minutes of video footage of a human driving the RC car, then trains a VAE to compress this information. He then uses a Soft Actor-Critic method with a carefully designed reward function and a penalty for non-smooth driving. Well done!</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://imageog.flaticon.com/icons/png/512/248/248114.png"
                       class="fa fa-fw category-logo">Production & Engineering
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://dev.to/robogeek/principled-machine-learning-4eho">Principled Machine Learning: Practices and Tools for Efficient Collaboration</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    David Herron
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://dev.to/robogeek/principled-machine-learning-4eho">
                          <img src=https://res.cloudinary.com/practicaldev/image/fetch/s--p4K5MDr8--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/5qw1fz7mk4guq9lp27z1.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <p>In this blog post, David Herron states the problem with current machine learning workflows: reproducibility, results sharing, transparency. After stating what an ideal solution should look like, he goes on to compare two popular alternatives: MLFlow and DVC. DVC appears to have his preference for its ease of use (transparent) and its flexibility (the pipelines we can generate with DVC resemble more of a directed acyclic graph rather than a sequence of steps as offered by MLFlow).</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/263/263051.png"
                       class="fa fa-fw category-logo">Everything Else
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://www.stateof.ai/">State of AI 2019</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Air Street Capital, UCL Institute for Innovation and Public Purpose
                  </h6>

                  
                                    <blockquote class="blockquote">
                     <p class="quote">In this report, we set out to capture a snapshot of the exponential progress in AI with a focus on developments in the past 12 months. Consider this report as a compilation of the most interesting things we’ve seen that seeks to trigger an informed conversation about the state of AI and its implication for the future.</p>
                  </blockquote>
                                    <p>This report is made out of slides with lots of visuals which makes it easy to go through if you are in a rush. It is divided in several sections (Research, Talent, Industry, Politics), each of them giving an interesting insight on the dynamics of artificial intelligence. Topics discussed are advances in reinforcement learning, natural language processing, medical application but also the imbalance men/women in the field, patents, hardware,...  </p>
              </div>
              <hr>

                          
          </div>
                </div>

  <!-- End Grid -->
  </div>

    
  <!-- End Page Container -->
</div>

</body>
</html>