<!DOCTYPE html>
<html>
<title>ML newsletter</title>
<meta charset="UTF-8">

<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<!-- Optional theme -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Roboto'>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<style>
	html,body,h1,h2,h3,h4,h5,h6 {
        font-family: "Roboto", sans-serif;
    }

    p {
        font-size: 15px;
    }

	.category{
	  padding: 20px;
	  margin: auto;
	  background: #F5F5F5;
	  margin-bottom: 25px;
	  box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
	  transition: all 0.3s; 
	  border-radius: 3px;
	}

    .category-title{
      margin-bottom: 30px;
    }

	.category-logo{
	  margin-right: 10px;
	}

	.article-logo{
	  margin-right: 3px;
	}

    .recommended{
	  margin-right: 2px;
    }

    .quote{
      font-family: "Times New Roman", serif;
      font-size: 15px;
    }
</style>


<body>

<!-- Page Container -->
<div class="container-fluid">

  
  <!-- The Grid -->
  <div class="row">
  
      <div class="col-md-10 col-md-offset-1">

          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/653/653500.png"
                       class="fa fa-fw category-logo">General Machine Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://medium.com/syncedreview/icml-2019-google-eth-zurich-mpi-is-cambridge-prowler-io-share-best-paper-honours-4aeabd5c9fc8">ICML 2019: best papers and honorable mentions</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Synced
                  </h6>

                  
                                    <p>It's ICML week! Two papers have been selected for the best paper awards: <a href=https://arxiv.org/abs/1811.12359>Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</a> and <a href=https://arxiv.org/pdf/1903.03571>Rates of Convergence for Sparse Variational Gaussian Process Regression</a>. You can also find the list of honorable mentions in the blog post.</p>
              </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/149/149125.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://www.youtube.com/channel/UCvqEpkx-HQ2nDMT-ob-AADg/videos">ICML 2019 Conference Videos</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://www.youtube.com/channel/UCvqEpkx-HQ2nDMT-ob-AADg/videos">
                          <img src=https://i.ibb.co/VWZRn3Y/Selection-832.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <p>This is the link you want to bookmark! Spotlight sessions and other talks are already being uploaded to this Youtube page.</p>
              </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://weightagnostic.github.io/">[arXiv] Weight Agnostic Neural Networks</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Inria, Google Brain
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://weightagnostic.github.io/">
                          <img src=https://i.ibb.co/n090QK2/Selection-831.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <blockquote class="blockquote">
                     <p class="quote">Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. <b>We propose a search method for neural network architectures that can already perform a task without any explicit weight training.</b> To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. <b>We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training.</b> On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights.</p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/149/149243.png"
                       class="fa fa-fw category-logo">Deep Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="http://gradientscience.org/robust_reps/">Robustness beyond Security: Representation Learning</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    MIT
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="http://gradientscience.org/robust_reps/">
                          <img src=http://gradientscience.org/assets/rf1_images/misclassification_IN.jpg
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <p>This blog post is an introduction to a recent MIT paper <a href=https://arxiv.org/abs/1906.00945>Learning Perceptually-Aligned Representations via Adversarial Robustness</a>. As the name suggests, authors train a network to <a href=http://gradientscience.org/robust_opt_pt1>reduce its sensitivity to adversarial examples</a> and observe that by doing so the network learn features that are more meaningful for a human. Several qualitative examples are shown. In a <a href=http://gradientscience.org/robust_apps/>second blog post</a>, authors expand on the idea and show that thanks to the better feature representation, the network can be used for image generation, super-resolution, in-painting and image-to-image translation. My personal favourite application is probably the image I associate with this article which shows potential reasons for the misclassification of a given image.</p>
              </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/25/25231.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub/">Towards Reproducible Research with PyTorch Hub</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Pytorch
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub/">
                          <img src=https://pytorch.org/assets/images/hub-blog-header-1.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <p>Pytorch had already a neat API to load standard architectures (VGG, ResNet, DenseNet, MobileNet, etc.) but the list of available architectures rarely grew and required a Pytorch update anyway. Because of this, <a href=https://github.com/Cadene/pretrained-models.pytorch>alternative 'model-zoo' projects</a> grew quite popular. Pytorch Hub is an attempt to standardise the practice of submitting an architecture and its pretrained weights to the Pytorch ecosystem. Now anyone can contribute to the official list of Pytorch builtin architectures. Some integrations are well-thought. For example, the <a href=https://pytorch.org/hub/pytorch_vision_densenet/>DenseNet page</a> provides instructions and examples on how to load the architecture alongside a Colab notebook to see it in action.</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/179/179121.png"
                       class="fa fa-fw category-logo">Optimisation & Learning Theory
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1905.12737">[arXiv] Less is More: An Exploration of Data Redundancy with Active Dataset Subsampling</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    NVIDIA
                  </h6>

                  
                                    <blockquote class="blockquote">
                     <p class="quote">Deep Neural Networks (DNNs) often rely on very large datasets for training. Given the large size of such datasets, it is conceivable that they contain certain samples that either do not contribute or negatively impact the DNN's performance. If there is a large number of such samples, subsampling the training dataset in a way that removes them could provide an effective solution to both improve performance and reduce training time. In this paper, we propose an approach called <Active Dataset Subsampling (ADS), to identify favorable subsets within a dataset for training using ensemble based uncertainty estimation. When applied to three image classification benchmarks (CIFAR-10, CIFAR-100 and ImageNet) <b>we find that there are low uncertainty subsets, which can be as large as 50% of the full dataset, that negatively impact performance.</b> These subsets are identified and removed with ADS. We demonstrate that datasets obtained using ADS with a lightweight ResNet-18 ensemble remain effective when used to train deeper models like ResNet-101. <b>Our results provide strong empirical evidence that using all the available data for training can hurt performance on large scale vision tasks.</b></p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/249/249156.png"
                       class="fa fa-fw category-logo">Reinforcement Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1906.00431?utm_campaign=RL%20Weekly&utm_medium=email&utm_source=Revue%20newsletter">[ICML 2019 Workshop] An Empirical Study on Hyperparameters and their Interdependence for RL Generalization</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    OpenAI
                  </h6>

                  
                                    <blockquote class="blockquote">
                     <p class="quote">Recent results in Reinforcement Learning (RL) have shown that agents with limited training environments are susceptible to a large amount of overfitting across many domains. A key challenge for RL generalization is to quantitatively explain the effects of changing parameters on testing performance. Such parameters include architecture, regularization, and RL-dependent variables such as discount factor and action stochasticity. <b>We provide empirical results that show complex and interdependent relationships between hyperparameters and generalization.</b> We further show that several empirical metrics such as gradient cosine similarity and trajectory-dependent metrics serve to provide intuition towards these results.</p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/484/484582.png"
                       class="fa fa-fw category-logo">Natural Language Processing
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1906.04341">[BlackBoxNLP 2019] What Does BERT Look At? An Analysis of BERT's Attention</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Stanford University, Facebook AI Reseach
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://arxiv.org/abs/1906.04341">
                          <img src=https://i.ibb.co/cbynkbd/Selection-834.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <blockquote class="blockquote">
                     <p class="quote">Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. <b>We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy.</b> Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.</p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

                          
          </div>
                </div>

  <!-- End Grid -->
  </div>

    
  <!-- End Page Container -->
</div>

</body>
</html>