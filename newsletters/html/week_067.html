<!DOCTYPE html>
<html>
<title>ML newsletter</title>
<meta charset="UTF-8">

<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<!-- Optional theme -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Roboto'>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<style>
	html,body,h1,h2,h3,h4,h5,h6 {
        font-family: "Roboto", sans-serif;
    }

    p {
        font-size: 15px;
    }

	.category{
	  padding: 20px;
	  margin: auto;
	  background: #F5F5F5;
	  margin-bottom: 25px;
	  box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
	  transition: all 0.3s; 
	  border-radius: 3px;
	}

	.intro-text{
	  padding: 20px;
	  margin: auto;
	  margin-bottom: 10px;
	  background: #F0EEEE;
	  box-shadow: 0 6px 12px rgba(0,0,0,0.16), 0 6px 12px rgba(0,0,0,0.23);
	  transition: all 0.3s; 
	}

    .category-title{
      margin-bottom: 30px;
    }

	.category-logo{
	  margin-right: 10px;
	}

	.article-logo{
	  margin-right: 3px;
	}

    .recommended{
	  margin-right: 2px;
    }

    .quote{
      font-family: "Times New Roman", serif;
      font-size: 15px;
    }
</style>


<body>

<!-- Page Container -->
<div class="container-fluid">

  
  <!-- The Grid -->
  <div class="row">
  
      <div class="col-md-10 col-md-offset-1">

          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/653/653500.png"
                       class="fa fa-fw category-logo">General Machine Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://blog.evjang.com/2019/07/likelihood-model-tips.html">Tips for Training Likelihood Models</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                        <span class="badge">
                                  Recommended
                              </span>
                                                    <a href=https://blog.evjang.com/>Eric Jang</a>
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://blog.evjang.com/2019/07/likelihood-model-tips.html">
                          <img src=https://lh3.googleusercontent.com/joXi6MzAiWxX9MtZR7QRMR6w7OFa5U1zQ9UR9odufiXQ3T6sVUXfcHDEcRjWm6NhWdG1TFhUEi22jxA0mv9pQ03A63uotmT1Sf2MUKaoYGaUgZ9H8Jj_M05nSx9rueAvfMK_70fU
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <p>This is quite a great blog post that offers either an introduction or a refresher on generative modelling: how the problem is formulated, which metrics can be used, what types of sampling mechanisms have been tried and a lot more. Eric Jang also includes plenty of references to dig deeper, be it blog posts, papers, <a href=https://statistics.stanford.edu/>Stanford links</a>,... Definitively a top resource to get a quick and decent overview of the field.</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/149/149243.png"
                       class="fa fa-fw category-logo">Deep Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://drsleep.github.io/NAS-at-CVPR-2019/">Neural Architecture Search at CVPR 2019</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    <a href=https://drsleep.github.io/>Vladimir Nekrasov</a>
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://drsleep.github.io/NAS-at-CVPR-2019/">
                          <img src=https://i.ibb.co/Y8hjMgN/Selection-941.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <p>Want to know what is cutting edge in the field of neural architecture search? This blog post will help you with this! You will find ~10 papers published at CVPR summarised in just one or two paragraphs.</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/722/722174.png"
                       class="fa fa-fw category-logo">Computer Vision
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1907.01949">[MICCAI 2019] Supervised Uncertainty Quantification for Segmentation with Multiple Annotations</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    University of Amsterdam, Radboud University Medical Center
                  </h6>

                  
                                    <blockquote class="blockquote">
                     <p class="quote">The accurate estimation of predictive uncertainty carries importance in medical scenarios such as lung node segmentation. Unfortunately, most existing works on predictive uncertainty do not return calibrated uncertainty estimates, which could be used in practice. In this work we exploit multi-grader annotation variability as a source of 'groundtruth' aleatoric uncertainty, which can be treated as a target in a supervised learning problem. We combine this groundtruth uncertainty with a Probabilistic U-Net and test on the LIDC-IDRI lung nodule CT dataset and MICCAI2012 prostate MRI dataset. We find that we are able to improve predictive uncertainty estimates. We also find that we can improve sample accuracy and sample diversity.</p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="http://www.robots.ox.ac.uk/~vgg/research/unsupervised_pose/">[CVPR 2019] Learning Landmarks from Unaligned Data using Image Translation</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    University of Oxford, DeepMind, University of Edinburgh, Facebook AI Research
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="http://www.robots.ox.ac.uk/~vgg/research/unsupervised_pose/">
                          <img src=http://www.robots.ox.ac.uk/~vgg/research/unsupervised_pose/img/model.jpg
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <blockquote class="blockquote">
                     <p class="quote">We introduce a method for learning landmark detectors from unlabelled video frames and unpaired labels. This allows us to learn a detector from a large collection of raw videos given only a few example annotations harvested from existing data or motion capture. We achieve this by formulating the landmark detection task as one of image translation, learning to map an image of the object to an image of its landmarks, represented as a skeleton. The advantage is that this translation problem can then be tackled by CycleGAN. However, we show that a naive application of CycleGAN confounds appearance and pose information, with suboptimal keypoint detection performance. We solve this problem by introducing an analytical and differentiable renderer for the skeleton image so that no appearance information can be leaked in the skeleton. Then, since cycle consistency requires to reconstruct the input image from the skeleton, we supply the appearance information thus removed by conditioning the generator with a second image of the same object (e.g. another frame from a video). Furthermore, while CycleGAN uses two cycle consistency constraints, we show that the second one is detrimental in this application and we discard it, significantly simplifying the model. We show that these modifications improve the quality of the learned detector leading to state-of-the-art unsupervised landmark detection performance in a number of challenging human pose and facial landmark detection benchmarks.</p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/179/179121.png"
                       class="fa fa-fw category-logo">Optimisation & Learning Theory
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="http://www.offconvex.org/2019/06/16/modeconnectivity/">Landscape Connectivity of Low Cost Solutions for Multilayer Nets</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    <a href=http://www.offconvex.org/>Off the convex path</a> - Rong Ge
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="http://www.offconvex.org/2019/06/16/modeconnectivity/">
                          <img src=http://www.offconvex.org/assets/modes.PNG
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <p>In this blog post, Rong Ge introduces a <a href=https://arxiv.org/abs/1906.06247>paper</a> he recently submitted, along with colleagues from Duke University and Princeton University. In the paper, researches explore the notion of <i>mode connectivity</i>, that is the relationship that links two optima of a loss function. In particular, prior works pointed out that given any two optimal set of weights for a given network and a given loss function, there exists a piece-wise linear combination of the weights that is an optimal solution as well. In the post, Dr Ge shows what such combination looks like when the network is said to be <i>dropout stable</i> (in the paper, they extend the result to <i>noisy stability</i> and <i>channel-wise dropout stability</i>). As he says, 'our methods do not answer all mysteries about mode connectivity [but] are a first-cut explanation for how mode connectivity can arise in realistic deep nets'. He made a good effort to make it easy to read for people who aren't comfortable with theory papers, so give it a go!</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/112/112272.png"
                       class="fa fa-fw category-logo">Gans & Adversarial Attacks
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://arxiv.org/abs/1907.02544">[arXiv] Large Scale Adversarial Representation Learning</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    DeepMind
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://arxiv.org/abs/1907.02544">
                          <img src=https://i.ibb.co/tY3h42g/Selection-940.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <blockquote class="blockquote">
                     <p class="quote">Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation.</p>
                  </blockquote>
                                    <p></p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/484/484582.png"
                       class="fa fa-fw category-logo">Natural Language Processing
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/25/25231.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://github.com/mihail911/nlp-library">NLP Library: curated collection of papers for the NLP practitioner</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Mihail Eric
                  </h6>

                  
                                    <p>A great little resource to explore important papers in NLP. It's simply organised and works well: a category, a paper, its link and a summary that fits in less than 3 lines.</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/56/56243.png"
                       class="fa fa-fw category-logo">Laugh Of The Week
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/8/8800.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://xkcd.com/2173/">"Yeah, I trained a neural net to sort the unlabeled photos into categories"</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    xkcd
                  </h6>

                                    <div style="padding-bottom: 15px;">
                      <a href="https://xkcd.com/2173/">
                          <img src=https://imgs.xkcd.com/comics/trained_a_neural_net.png
                               class="img-responsive center-block"
                               style="max-width: 80%;
                                      max-height: 300px;
                                      border: 1px solid #A5A5A5;">
                      </a>
                  </div>
                  
                                    <p></p>
              </div>
              <hr>

                          
          </div>
                </div>

  <!-- End Grid -->
  </div>

    
  <!-- End Page Container -->
</div>

</body>
</html>