<!DOCTYPE html>
<html>
<title>ML newsletter</title>
<meta charset="UTF-8">

<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<!-- Optional theme -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Roboto'>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<style>
	html,body,h1,h2,h3,h4,h5,h6 {font-family: "Roboto", sans-serif}

	.category{
	  padding: 20px;
	  margin: auto;
	  background: #F5F5F5;
	  margin-bottom: 25px;
	  box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
	  transition: all 0.3s; 
	  border-radius: 3px;
	}

    .category-title{
      margin-bottom: 30px;
    }

	.category-logo{
	  margin-right: 10px;
	}

	.article-logo{
	  margin-right: 3px;
	}

    .recommended{
	  margin-right: 2px;
    }

    .quote{
      font-size: 11px;
    }
</style>


<body>

<!-- Page Container -->
<div class="container" style="max-width:1400px;">

  
  <!-- The Grid -->
  <div class="row">
  
      <!-- Dummy container to force centering. Yes. I'm a front-end noob. -->
      <div class="col-sm-2"></div>

      <div class="col-sm-8">

          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/653/653500.png"
                       class="fa fa-fw category-logo">General Machine Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <b><a href="http://tools.google.com/seedbank/">Seedbank: Collection of Interactive Machine Learning Examples</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                        <span class="badge">
                                  Recommended
                              </span>
                                                    Google
                  </h6>
                                    <p>For a little while, Google has been providing <a href=https://colab.research.google.com/notebooks/welcome.ipynb#recent=true>Colab notebooks</a>, a jupyter-like interface hosted by Google itself. The GPU computing resources are also provided for free. That is huge by itself! Now, Seedbank's intent is to gather the best of Colab notebooks dedicated to machine learning. For all these diverse examples (lucid dream, MNIST classification, attention recurrent neural net and translation, GANs, VAEs for music generation and others), you can actually run and play around with the code! All you need is a web browser...</p>
              </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/155/155292.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://david-abel.github.io/blog/posts/misc/icml_2018.pdf">ICML 2018 Notes</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    David Abel (Brown University)
                  </h6>
                                    <p>In February, Mr. Abel was already amazingly sharing 82 pages of notes on the AAAI conference. Six months later, he does it again with ICML! Note that the notes are mainly targeted towards reinforcement learning.</p>
              </div>
              <hr>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/149/149125.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://www.youtube.com/watch?v=nq6iPZVUxZU">UMAP: oral presentation at the SciPy conference</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Leland McInnes (Tutte Institute for Mathematics and Computing)
                  </h6>
                                    <p>Maybe you have already heard of UMAP, a dimentionality reduction technique that is gaining more and more popularity over t-SNE. In this presentation given by author, you'll learn the intuition behind UMAP and the different aspects that come into play. <br /> If you're interested in using UMAP, note that Dr. McInnes just released the new version 0.3 of his package (<a href=https://twitter.com/leland_mcinnes/status/1017519182060105734>more info</a>).</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/149/149243.png"
                       class="fa fa-fw category-logo">Deep Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://sahnimanas.github.io/2018/06/24/quantization-in-tf-lite.html">Making Neural Nets Work With Low Precision: 8-Bit Quantization and TensorFlow-Lite</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Manas Sahni (Delhi Technological University)
                  </h6>
                                    <p>An interesting introduction on quantization and how to 'convert' a network working with floats to work with 8-bits integers. As the author clearly explains, this involves a few challenges especially when we don't know the range of some of the values dealt with by the network (typically: the input of all layers). Additionally, he details what strategy is used to perform quantization in Tensorflow-Lite.</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/179/179121.png"
                       class="fa fa-fw category-logo">Optimisation & Learning Theory
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="http://www.nowozin.net/sebastian/blog/drafts/do-bayesians-overfit.html">Do Bayesians Overfit?</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                        <span class="badge">
                                  Recommended
                              </span>
                                                    Sebastian Nowozin (Microsoft Research)
                  </h6>
                                    <p>An interesting read on a topic often neglected: overfitting of bayesian models. Here Dr. Nowozin provides a measure of overfitting in terms of a bayesian model and first uses it in a toy-example where all the distributions have closed-form solutions. And he does observe overfitting although it is limited compared to the Maximum Aposteriori estimator (MAP) and the Maximum Likelihood estimator (MLE). More importantly, based on a result by <a href=https://www.cambridge.org/core/books/algebraic-geometry-and-statistical-learning-theory/9C8FD1BDC817E2FC79117C7F41544A3A#>Watanabe</a>, he shows that we can approximate the bayesian measure of overfitting in O(1/n^2) where n is the number of samples!</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/249/249156.png"
                       class="fa fa-fw category-logo">Reinforcement Learning
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/174/174858.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://medium.com/@awjuliani/on-solving-montezumas-revenge-2146d83f0bc3">On “solving” Montezuma’s Revenge</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Arthur Juliani (Unity 3D)
                  </h6>
                                    <p>Two weeks ago, we were mentioning the notable achievement of OpenAI which <a href=https://blog.openai.com/learning-montezumas-revenge-from-a-single-demonstration/>managed to solve Montezuma's Revenge</a> with a high score. While DeepMind also managed to solve the problem, both solutions were fundamentally based on human demonstration. In this blog post, the author argues that Montezuma's Revenge shouldn't be considered 'solved' just yet. He offers a summary of the solutions proposed by DeepMind and OpenAI and advocates for agents that would be able to <i>reason</i>.</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://imageog.flaticon.com/icons/png/512/248/248114.png"
                       class="fa fa-fw category-logo">Production & Engineering
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/174/174858.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="https://medium.com/tensorflow/autograph-converts-python-into-tensorflow-graphs-b2a871f87ec7?linkId=54396538">AutoGraph converts Python into TensorFlow graphs</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Google
                  </h6>
                                    <blockquote class="blockquote">
                     <p class="quote">AutoGraph takes in your eager-style Python code and converts it to graph-generating code.</p>
                  </blockquote>
                                    <p>Tensorflow is really impressive by the extent of its capabilities. This new feature called AutoGraph builds up on that. Previously, Tensorflow introduced <a href=https://www.tensorflow.org/guide/eager>eager execution</a> that allowed to avoid the often tricky step of building a computation graph (like Pytorch does). What we are loosing is probably speed of execution and ease of distributing the computation. Now fear no more as AutoGraph comes to the rescue. It is basically a converter that is going to interpret the eager-code and reformulate it in terms of a graph. A range of commands are supported (loops and loop-control operations such as <i>continue</i> or <i>break</i>, conditions, prints, appending) making it extremely versatile in theory. I am really looking forward to see applications of AutoGraph!</p>
              </div>
              <hr>

                          
          </div>
          
          <!-- Category block -->
          <div class="category">
              <h2 class="category-title">
                  <img src="https://image.flaticon.com/icons/png/512/263/263051.png"
                       class="fa fa-fw category-logo">Everything Else
              </h2>

              
              <div class="article">
                  <h5 class="article-title">
                                            <img src="https://image.flaticon.com/icons/png/512/60/60736.png"
                           class="fa fa-fw article-logo">
                                            <b><a href="http://seaborn.pydata.org/whatsnew.html#v0-9-0-july-2018">Seaborn 0.9</a></b>
                  </h5>

                  <h6 class="w3-text">
                                                    Seaborn
                  </h6>
                                    <p>A new version of this popular visualisation library is out! New plots have been added including (some would say 'finally') the <a href=http://seaborn.pydata.org/generated/seaborn.scatterplot.html#seaborn.scatterplot>scatter plot</a> and <a href=http://seaborn.pydata.org/generated/seaborn.lineplot.html#seaborn.lineplot>line plot</a>. Happy plotting!</p>
              </div>
              <hr>

                          
          </div>
                </div>
      
  <!-- Dummy container to force centering. Yes. I'm a front-end noob. -->
  <div class="col-sm-2"></div>

  <!-- End Grid -->
  </div>

    
  <!-- End Page Container -->
</div>

</body>
</html>