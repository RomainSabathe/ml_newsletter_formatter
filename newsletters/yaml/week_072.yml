articles:
  - category: ml
    description: 'The title says it all. Notice that for now we only have access to
      the title of the papers and not their content.'
    lab: ""
    title: 'NeurIPS 2019: list of accepted papers'
    type: paper
    url: https://neurips.cc/Conferences/2019/AcceptedPapersInitial
  - category: theory
    description:
    img: https://i.ibb.co/Yp6mJws/Selection-964.png
    lab: 'Mila, Universite de Montreal, University of British Columbia'
    quote: 'Recent works have shown that stochastic gradient descent (SGD) achieves
      the fast convergence rates of full-batch gradient descent for over-parameterized
      models satisfying certain interpolation conditions. However, the step-size used
      in these works depends on unknown quantities, and SGD''s practical performance
      heavily relies on the choice of the step-size. We propose to use line-search
      methods to automatically set the step-size when training models that can interpolate
      the data. We prove that SGD with the classic Armijo line-search attains the
      fast convergence rates of full-batch gradient descent in convex and strongly-convex
      settings. We also show that under additional assumptions, SGD with a modified
      line-search can attain a fast rate of convergence for non-convex functions.
      Furthermore, we show that a stochastic extra-gradient method with a Lipschitz
      line-search attains a fast convergence rate for an important class of non-convex
      functions and saddle-point problems satisfying interpolation. We then give heuristics
      to use larger step-sizes and acceleration with our line-search techniques. We
      compare the proposed algorithms against numerous optimization methods for standard
      classification tasks using both kernel methods and deep networks. The proposed
      methods are robust and result in competitive performance across all models and
      datasets. Moreover, for the deep network models, SGD with our line-search results
      in both faster convergence and better generalization.'
    title: "[NeurIPS 2019] Painless Stochastic Gradient: Interpolation, Line-Search,\
      \ and Convergence Rates"
    type: paper
    url: https://arxiv.org/abs/1905.09997
  - category: theory
    description:
    img: https://i.ibb.co/Cty3vJk/Selection-965.png
    lab: 'Facebook AI Research, University of Pennsylvania'
    quote: 'The success of deep networks has been attributed in part to their expressivity:
      per parameter, deep networks can approximate a richer class of functions than
      shallow networks. In ReLU networks, the number of activation patterns is one
      measure of expressivity; and the maximum number of patterns grows exponentially
      with the depth. However, recent work has showed that the practical expressivity
      of deep networks - the functions they can learn rather than express - is often
      far from the theoretical maximum. In this paper, we show that the average number
      of activation patterns for ReLU networks at initialization is bounded by the
      total number of neurons raised to the input dimension. We show empirically that
      this bound, which is independent of the depth, is tight both at initialization
      and during training, even on memorization tasks that should maximize the number
      of activation patterns. <b>Our work suggests that realizing the full expressivity
      of deep networks may not be possible in practice, at least with current methods.</b> '
    title: "[NeurIPS 2019] Deep ReLU Networks Have Surprisingly Few Activation Patterns"
    type: paper
    url: https://arxiv.org/abs/1906.00904
  - category: dl
    description:
    img: https://i.ibb.co/7XLG7dz/Selection-966.png
    lab: 'Georgia Tech'
    quote: "[..] Existing work on interpreting neural network predictions for images\
      \ often focuses on explaining predictions for single images or neurons. As predictions\
      \ are often computed from millions of weights that are optimized over millions\
      \ of images, such explanations can easily miss a bigger picture. We present\
      \ Summit, an interactive system that scalably and systematically summarizes\
      \ and visualizes what features a deep learning model has learned and how those\
      \ features interact to make predictions. Summit introduces two new scalable\
      \ summarization techniques: (1) activation aggregation discovers important neurons,\
      \ and (2) neuron-influence aggregation identifies relationships among such neurons.\
      \ Summit combines these techniques to create the novel attribution graph that\
      \ reveals and summarizes crucial neuron associations and substructures that\
      \ contribute to a model's outcomes. Summit scales to large data, such as the\
      \ ImageNet dataset with 1.2M images, and leverages neural network feature visualization\
      \ and dataset examples to help users distill large, complex neural network models\
      \ into compact, interactive visualizations. <b>We present neural network exploration\
      \ scenarios where Summit helps us discover multiple surprising insights into\
      \ a prevalent, large-scale image classifier's learned representations and informs\
      \ future neural network architecture design.</b> The Summit visualization runs\
      \ in modern web browsers and is open-sourced. "
    title: "[IEEE VAST 2020] Summit: Scaling Deep Learning Interpretability by Visualizing\
      \ Activation and Attribution Summarizations"
    type: paper
    url: https://arxiv.org/abs/1904.02323v3
  - category: dl
    description: 'See also the <a href=https://github.com/PGM-Lab/InferPy/>GitHub
      link</a>.'
    img: https://i.ibb.co/VYRknTq/Selection-967.png
    lab: 'University of Almeria, IDSIA'
    quote: 'InferPy is a Python package for probabilistic modeling with deep neural
      networks. InferPy defines a user-friendly API which trades-off model complexity
      with ease of use, unlike other libraries whose focus is on dealing with very
      general probabilistic models at the cost of having a more complex API. In particular,
      Inferpy allows to define, learn and evaluate general hierarchical probabilistic
      models containing deep neural networks in a compact and simple way. InferPy
      is built on top of Tensorflow, Edward2 and Keras. "'
    title: "[arXiv] InferPy: Probabilistic Modeling with Deep Neural Networks Made\
      \ Easy"
    type: github
    url: https://arxiv.org/abs/1908.11161v2
intro_text: ""
outro_text: ""
