intro_text:

outro_text:

articles:
    - title: "Notes on the Reinforcement Learning Summer School (2017)"
      url: "https://yobibyte.github.io/rlss17.html#rlss17"
      type: "blog"
      category: "rl"
      lab: "Vitaly Kurin"
      description: "This is not a recent post but it has been directly sent to me; and the amount of work is impressive. This can be basically seen as a webpage summarizing the entirety of the 2017th edition of the Reinforcement Learning Summer School by the CIFAR Institute. The writing style is a bit on the note-taking side than on the explantion side but considering Mr. Kurin also provides the links to the slides and videos of each presentation he covers, I think this page could serves as a wonderful helping material if you decide to go through the presentations yourself."
      quote: ""
      recommended: "True"

    - title: "Acamedic Torrents"
      url: "http://academictorrents.com/"
      type: ""
      category: ""
      lab: ""
      description: "27 terabytes worth of datasets, papers and courses shared via the torrent protocol. You will find classic material (Andrew Ng's Coursera course, ImagetNet, MNIST) and some more exotic (Labeled Fishes in the Wild, Movies Fight Detection Dataset,...)"
      quote: ""
      recommended: ""

    - title: "Interactive Machine Learning List"
      url: "https://p.migdal.pl/interactive-machine-learning-list/"
      type: "github"
      category: ""
      lab: "Piotr Migdał"
      description: "A collection of all the interactive ML demos you can find online. Well, maybe not all of them, but a fair number. And the project is open-source so anyone is invited to contribute to the list. "
      quote: ""
      recommended: ""

    - title: "Now anyone can train Imagenet in 18 minutes"
      url: "http://www.fast.ai/2018/08/10/fastai-diu-imagenet/"
      type: "blog"
      category: "ml"
      lab: "fast.ai"
      description: "This article was quite discussed this week as it has been covered by some mainstream technology media. A group of 3 from fast.ai (Andrew Shaw, Yaroslav Bulatov and Jeremy Howard) managed to train a ResNet 50 on ImagetNet from scratch and to reach 93+% top-5 accuracy in 18 minutes for about $40. This article explains their approach. I wish we had a bit more details on the results; in particular an ablation study of the different tricks they used (using rectangular images, proressive resizing, dynamic batch sizes...)."
      quote: ""
      recommended: ""

    - title: "Neural Processes as distributions over functions"
      url: "http://kasparmartens.rbind.io/post/np/"
      type: "blog"
      category: "ml"
      lab: "Kaspar Märtens"
      description: "If you don't know what neural processes are, this blog post is a great place to start. This concept which builds on the ideas of deep learning and gaussian processes was recently explored by DeepMind in their <a href=https://arxiv.org/abs/1807.01622>ICML 2018 workshop publication</a>. The idea is quite audacious I thought as it models each weight in a network as a Gaussian distribution. This gives you virtually an infinite number of deep networks; only a few datapoints should suffice to sample one specific network that will be well-suited for these datapoints (i.e. for the task at hand)."
      quote: ""
      recommended: "True"

    - title: "OpenAI & DOTA 2: Game Is Hard"
      url: "https://github.com/lyndonzheng/Synthetic2Realistic"
      type: "blog"
      category: "rl"
      lab: "Michael Cook"
      description: "Nothing technical in this blog post; however the author who is a regular DOTA 2 player explains in details and with examples why the 3 games played by OpenAI last week were so surprising. He analyses the apparent behavior of the bot players and makes it simple to understand for those who are not familiar with the game!"
      quote: ""
      recommended: ""

    - title: "The four kinds of (young) deep learners"
      url: "https://www.facebook.com/722677142/posts/10155453249732143/"
      type: ""
      category: "laugh"
      lab: ""
      description: ""
      quote: ""
      recommended: ""

    - title: "Training Robust Classifiers"
      url: "http://people.csail.mit.edu/madry/lab/blog/adversarial/2018/08/10/robust_optimization_part2/"
      type: "blog"
      category: "gan"
      lab: "gradient science (Aleksander Mądry, Ludwig Schmidt, Dimitris Tsipras)"
      description: "This is the second part of a collection of blog posts targeted towards adversarial training and robustness of classifiers againt adversarial perturbations; the first part can be found <a href=http://people.csail.mit.edu/madry/lab/blog/adversarial/2018/07/11/robust_optimization_part1/>here</a>. The explanations are clear and I liked that the authors presented the results of their experiments while giving an overview of the topic of robustness. We learn about the min-max formulation of this problem, Danskin's theorem and how to apply it (using Projected Gradient Descent) and how former methods such as Fast Gradient Sign Method (FGSM) were seemingly not following the theorem."
      quote: ""
      recommended: ""

    - title: "Large-Scale Study of Curiosity-Driven Learning"
      url: "https://pathak22.github.io/large-scale-curiosity/"
      type: "paper"
      category: "rl"
      lab: "OpenAI, UC Berkeley, University of Edinburgh"
      description: "In reinforcement learning, most of the reward functions are tailored for the task at hand. For instance, in the <a href=https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947>car pole balancing</a> problem, the reward value is traditionally based on the angle of the pole to the vertical (if it's too high, the reward could be negative for instance). Designing a reward function for each problem is not scalable and time consuming. On the contrary, this work explores reward functions which are <i>not</i> based on the problem at hand. Rather, in intuitive terms, authors try to model <i>curiosity</i> and encourage the network not to be bored!"
      quote: ""
      recommended: ""

    - title: "Graph Attention Networks"
      url: "http://petar-v.com/GAT/"
      type: "blog+paper"
      category: "ml"
      lab: "University of Cambridge, UAB, Montreal Institute for Learning Algorithms"
      description: "More and more work is dedicated to applying the tools of deep learning on computer vision to graph structures. As this blog post explains clearly, this problem is far from being trivial. The paper related in the blog post, titled Graph Attention Networks was published at ICLR this year and offers a clever solution. In this blog post, authors summarise the proposed solution and point to other works which extend their idea."
      quote: ""
      recommended: ""

    - title: "Model Scheduling"
      url: "http://nsaphra.github.io/post/model-scheduling/"
      type: "blog"
      category: "optimisation"
      lab: "Naomi Saphra (Objective Funk)"
      description: "This blog post is an extended litterature review on the topic of <i>model scheduling</i> which is here described as anything that has to do with modifying hyper-parameters of a model during training. The covered themes range from dropout to architecture growth, not to mention pruning and teacher/student techniques."
      quote: ""
      recommended: ""

    - title: "Transfer Learning in NLP"
      url: "https://blog.feedly.com/transfer-learning-in-nlp/"
      type: "blog"
      category: "nlp"
      lab: "Peter Martigny"
      description: "This is a rather high-level introduction of the paper <a href=https://arxiv.org/abs/1801.06146>
      Universal Language Model Fine-tuning for Text Classification</a> by Howard & Ruder in which they present a transfer learning method specifically for NLP. The claims are quite impressive as they report same performance when fine-tuning with 100 samples than when training from scratch with 10,000 samples. The author of the blog post also provides notebooks of experiments he did on his own to test this method on the Amazon Reviews dataset."
      quote: ""
      recommended: ""
