intro_text:

outro_text:

articles:
    - title: "Stats on ICLR 2019 submissions - an interactive notebook"
      url: "https://colab.research.google.com/drive/1jG5ilLQUvxvZ-HB60ovc4Ve_UK5ICoFT#scrollTo=h8tXLW0F-Fh8&forceEdit=true&offline=true&sandboxMode=true"
      type: ""
      category: ""
      lab: ""
      description: "This <i>colab notebook</i> lets you explore the statistics of ICLR 2019 submissions. Since it's hosted online, you can actually modify the code on the go and see the changes for yourself. Reinforcement learning, GANs and RNNs are topping the charts of the most submitted topics."
      quote: ""
      recommended: ""

    - title: "Pytorch 1.0 announcement"
      url: "https://code.fb.com/ai-research/facebook-accelerates-ai-development-with-new-partners-and-production-capabilities-for-pytorch-1-0/"
      type: ""
      category: "engineering"
      lab: "Facebook"
      description: "This news was hard to miss but here we are: the developer preview of Pytorch 1.0 is out. The ambition of this release has always been described as getting Pytorch 'production-ready'. It will be providing tools to transform your pytorch code into an actual computation graph, JIT compilation and a C++ interface to easily write high-performance code. Facebook also announced a number of good news for the Pytorch infrastructure with the official support of TPUs and Google Cloud as well as Azure. IBM, NVIDIA, Intel and Qualcom also announced they would provide kernel library integrations to directly optimise Pytorch code."
      quote: ""
      recommended: ""

    - title: ""
      url: "https://gpytorch.ai/"
      type: ""
      category: ""
      lab: ""
      description: ""
      quote: ""
      recommended: ""

    - title: "A look at Image Segmentation using CNNs"
      url: "https://mohitjain.me/2018/09/30/a-look-at-image-segmentation/"
      type: "blog"
      category: "cv"
      lab: "Mohit Jain"
      description: "This post offers a global introduction to the problem of image segmentation (and instance-aware semantic segmentation). The focus is on the different architectures that have been used over the year. The author does a good job at explaining the incremental changes that happened from the FCN architecture, all the way to SegNet and Mask R-CNN. Overall it's a good read if you are looking for an introduction to deep-learning-based image segmentation."
      quote: ""
      recommended: ""

    - title: "An Intuitive Guide to Optimal Transport, Part III: Entropic Regularization and the Sinkhorn Iterations"
      url: "https://www.mindcodec.com/an-intuitive-guide-to-optimal-transport-part-iii-entropic-regularization-and-sinkhorn-divergences/"
      type: "blog"
      category: "theory"
      lab: "Luca Ambrogioni"
      description: "This is the third blog post dedicated to optimal transport by Luca Ambrogioni. We linked the <a href=https://www.mindcodec.com/an-intuitive-guide-to-optimal-transport-for-machine-learning/>first</a> and <a href=https://www.mindcodec.com/an-intuitive-guide-to-optimal-transport-part-ii-the-wasserstein-gan-made-easy/>second</a> posts last week. This time the emphasis is on how to <i>smoothen</i> the solutions of the optimal transport problem. As the author explains, such solutions tend to be sparse which can cause collapses or lack of diversity in OT-based models like the Wasserstein GAN. As before, the explanations are top-notch and you'll learn how to derive the Sinkhorn algorithm in no time! "
      quote: ""
      recommended: "true"

    - title: "Multi-armed bandit implementation"
      url: "http://peterroelants.github.io/posts/multi-armed_bandit_implementation/"
      type: "blog"
      category: "ml"
      lab: "Peter Roelants"
      description: "This is a short hands-on introduction to the famous multi-armed bandit problem. Snippets of codes are provided so you can play around. I also liked  the <i>further readings</i> section that invites you to learn more about the <a href=http://varianceexplained.org/statistics/beta_distribution_and_baseball/>beta distribution</a> and <a href=https://www.evanmiller.org/bayesian-ab-testing.html#implementation>how to use Bayes when doing A/B testing</a>."
      quote: ""
      recommended: ""

    - title: "AI still fails on robust handwritten digit recognition (and how to fix it)"
      url: "https://medium.com/bethgelab/ai-still-fails-on-robust-handwritten-digit-recognition-and-how-to-fix-it-a432d84ede18"
      type: "blog"
      category: "gan"
      lab: "Wieland Brendel, University of Tubingen "
      description: "This blog post serves as an introduction to the paper <a href=https://arxiv.org/abs/1805.09190>Towards the first adversarially robust neural network model on MNIST</a> published by Schott et al. at ICLR 2018. The author presents a classification method that aims to be resistant against adversarial attacks. The core idea relies on building an ensemble of variational autoencoders, one for each class. When an image is given at test time, all the VAEs try to reproduce this image. The classification decision is given by the VAE that reproduced the test image best. See the blog post for more detailed explanations."
      quote: ""
      recommended: ""

    - title: "A review of the neural history of Natural Language Processing"
      url: "http://blog.aylien.com/a-review-of-the-recent-history-of-natural-language-processing/"
      type: "blog"
      category: "nlp"
      lab: "Sebastian Ruder"
      description: "If you are new to the field of NLP or just want to learn more, this could be a great place to start. We get a chronological list of the most famous neural-based NLP models as well as the focus points of research over the years. Although the author does not explain in many details all the models (otherwise it would become a book!), he provides tons of links to papers and blog posts for us to read."
      quote: ""
      recommended: ""

    - title: "Convolution: an exploration of a familiar operator's deeper roots"
      url: "https://towardsdatascience.com/convolution-a-journey-through-a-familiar-operators-deeper-roots-2e3311f23379"
      type: "medium"
      category: "ml"
      lab: "Cody Marie Wild"
      description: "If you have always used convolutions only through the scope of CNNs, then you will be interested in reading this post. We learn different aspects of the convolution operator (its function-smoothing effect; its ability to aggregate information) as well as the difference between the actual mathematical definition of convolution, and the convolution that is used in CNNs. In any case, it can serve as a good refresher."
      quote: ""
      recommended: ""
