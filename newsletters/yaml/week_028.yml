intro_text:

outro_text:

articles:
    - title: "ICML 2018 Notes"
      url: "https://david-abel.github.io/blog/posts/misc/icml_2018.pdf"
      type: "paper"
      category: "ml"
      lab: "David Abel (Brown University)"
      description: "In February, Mr. Abel was already amazingly sharing 82 pages of notes on the AAAI conference. Six months later, he does it again with ICML! Note that the notes are mainly targeted towards reinforcement learning."
      quote: ""
      recommended: ""

    - title: "UMAP: oral presentation at the SciPy conference"
      url: "https://www.youtube.com/watch?v=nq6iPZVUxZU"
      type: "video"
      category: "ml"
      lab: "Leland McInnes (Tutte Institute for Mathematics and Computing)"
      description: "Maybe you have already heard of UMAP, a dimentionality reduction technique that is gaining more and more popularity over t-SNE. In this presentation given by author, you'll learn the intuition behind UMAP and the different aspects that come into play. <br /> If you're interested in using UMAP, note that Dr. McInnes just released the new version 0.3 of his package (<a href=https://twitter.com/leland_mcinnes/status/1017519182060105734>more info</a>)."
      quote: ""
      recommended: ""

    - title: "Seaborn 0.9"
      url: "http://seaborn.pydata.org/whatsnew.html#v0-9-0-july-2018"
      type: "blog"
      category: "data"
      lab: "Seaborn"
      description: "A new version of this popular visualisation library is out! New plots have been added including (some would say 'finally') the <a href=http://seaborn.pydata.org/generated/seaborn.scatterplot.html#seaborn.scatterplot>scatter plot</a> and <a href=http://seaborn.pydata.org/generated/seaborn.lineplot.html#seaborn.lineplot>line plot</a>. Happy plotting!"
      quote: ""
      recommended: ""

    - title: "AutoGraph converts Python into TensorFlow graphs"
      url: "https://medium.com/tensorflow/autograph-converts-python-into-tensorflow-graphs-b2a871f87ec7?linkId=54396538"
      type: "medium"
      category: "engineering"
      lab: "Google"
      description: "Tensorflow is really impressive by the extent of its capabilities. This new feature called AutoGraph builds up on that. Previously, Tensorflow introduced <a href=https://www.tensorflow.org/guide/eager>eager execution</a> that allowed to avoid the often tricky step of building a computation graph (like Pytorch does). What we are loosing is probably speed of execution and ease of distributing the computation. Now fear no more as AutoGraph comes to the rescue. It is basically a converter that is going to interpret the eager-code and reformulate it in terms of a graph. A range of commands are supported (loops and loop-control operations such as <i>continue</i> or <i>break</i>, conditions, prints, appending) making it extremely versatile in theory. I am really looking forward to see applications of AutoGraph!"
      quote: "AutoGraph takes in your eager-style Python code and converts it to graph-generating code."
      recommended: ""

    - title: "Do Bayesians Overfit?"
      url: "http://www.nowozin.net/sebastian/blog/drafts/do-bayesians-overfit.html"
      type: "blog"
      category: "theory"
      lab: "Sebastian Nowozin (Microsoft Research)"
      description: "An interesting read on a topic often neglected: overfitting of bayesian models. Here Dr. Nowozin provides a measure of overfitting in terms of a bayesian model and first uses it in a toy-example where all the distributions have closed-form solutions. And he does observe overfitting although it is limited compared to the Maximum Aposteriori estimator (MAP) and the Maximum Likelihood estimator (MLE). More importantly, based on a result by <a href=https://www.cambridge.org/core/books/algebraic-geometry-and-statistical-learning-theory/9C8FD1BDC817E2FC79117C7F41544A3A#>Watanabe</a>, he shows that we can approximate the bayesian measure of overfitting in O(1/n^2) where n is the number of samples!"
      quote: ""
      recommended: "True"

    - title: "On “solving” Montezuma’s Revenge"
      url: "https://medium.com/@awjuliani/on-solving-montezumas-revenge-2146d83f0bc3"
      type: "medium"
      category: "rl"
      lab: "Arthur Juliani (Unity 3D)"
      description: "Two weeks ago, we were mentioning the notable achievement of OpenAI which <a href=https://blog.openai.com/learning-montezumas-revenge-from-a-single-demonstration/>managed to solve Montezuma's Revenge</a> with a high score. While DeepMind also managed to solve the problem, both solutions were fundamentally based on human demonstration. In this blog post, the author argues that Montezuma's Revenge shouldn't be considered 'solved' just yet. He offers a summary of the solutions proposed by DeepMind and OpenAI and advocates for agents that would be able to <i>reason</i>."
      quote: ""
      recommended: ""

    - title: "Making Neural Nets Work With Low Precision: 8-Bit Quantization and TensorFlow-Lite"
      url: "https://sahnimanas.github.io/2018/06/24/quantization-in-tf-lite.html"
      type: "blog"
      category: "dl"
      lab: "Manas Sahni (Delhi Technological University)"
      description: "An interesting introduction on quantization and how to 'convert' a network working with floats to work with 8-bits integers. As the author clearly explains, this involves a few challenges especially when we don't know the range of some of the values dealt with by the network (typically: the input of all layers). Additionally, he details what strategy is used to perform quantization in Tensorflow-Lite."
      quote: ""
      recommended: ""

    - title: "Seedbank: Collection of Interactive Machine Learning Examples"
      url: "http://tools.google.com/seedbank/"
      type: ""
      category: "ml"
      lab: "Google"
      description: "For a little while, Google has been providing <a href=https://colab.research.google.com/notebooks/welcome.ipynb#recent=true>Colab notebooks</a>, a jupyter-like interface hosted by Google itself. The GPU computing resources are also provided for free. That is huge by itself! Now, Seedbank's intent is to gather the best of Colab notebooks dedicated to machine learning. For all these diverse examples (lucid dream, MNIST classification, attention recurrent neural net and translation, GANs, VAEs for music generation and others), you can actually run and play around with the code! All you need is a web browser..."
      quote: ""
      recommended: "True"
