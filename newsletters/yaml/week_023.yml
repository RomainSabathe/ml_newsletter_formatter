intro_text:

outro_text:

articles:
    - title: "(Oral CVPR 2018) StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation"
      url: "https://arxiv.org/abs/1711.09020"
      type: "paper + gihub"
      category: "gan"
      lab: "Korea University, Clova AI Research, The College of New Jersey, Hong Kong University of Science & Technologies"
      description: "The novelty of this paper is that it proposes a single model to multiple multiple types of image-to-image translation (i.e. transfer to different domains). Being able to handle multiple domains has been a challenge or a while with GANs. The immediate benefit is that training can be done using multiple types of datasets; in the paper, the example presented is one dataset for gender/hair color and the other for facial emotion. An implemenation is given <a href=https://github.com/taki0112/StarGAN-Tensorflow>here</a>."
      quote: ""
      recommended: "True"

    - title: "Deep Reinforcement Learning course with Tensorflow"
      url: "https://simoninithomas.github.io/Deep_reinforcement_learning_Course/"
      type: "blog"
      category: "rl"
      lab: "Thomas Simonini"
      description: "This is another set of tutorials for deep reinforcement learning."
      quote: ""
      recommended: ""

    - title: "Simple tutorials on deep learning using TensorFlow Eager"
      url: "https://github.com/madalinabuzau/tensorflow-eager-tutorials"
      type: "github"
      category: "ml"
      lab: "Madalina Buzau"
      description: "A set of 9 Jupyter notebooks as tutorials to explain how to use Tensorflow Eager! The code is extremely clear and clean. It also shows how to make the best of the whole Keras API which is now ubiquitous in modern Tensorflow."
      quote: ""
      recommended: "True"

    - title: "Improving Language Understanding with Unsupervised Learning"
      url: "https://blog.openai.com/language-unsupervised/"
      type: "blog + paper"
      category: "nlp"
      lab: "OpenAI"
      description: "In this article, authors explain how they significatively improved performance over SOTA on a variety of NLP benchmarks. Interestingly, the key is not a better model or a clever trick but rather a better initialization. More precisely, 8 GPUs worked for a month on a very large corpus of texts to train a model in an unsupervised manner. Fine tuning from there leads to impressive improvement compared to training from a random initialization (~+55% relative performance for question answering, for example). I particularly liked that they were open and direct on the drawbacks of their method as well as the remaining open questions. <br /> Hey, know what? I started mentioning this article at the beginning of the week. And by Friday, we already have some open-source implementations available: <a href=https://github.com/huggingface/pytorch-openai-transformer-lm>here</a>."
      quote: ""
      recommended: ""

    - title: "ModelZoo"
      url: "https://modelzoo.co/"
      type: ""
      category: ""
      lab: "Jing Yu"
      description: "A compilation of the best GitHub projects that provide a pre-trained model, with a nice Bootstrap interface. Projects can be categorized by framework and application. It's pretty neat as a way to skim through available models. However, for downloading and implementing the model, you'll still have to go to the corresponding GitHub page."
      quote: ""
      recommended: ""

    - title: "Abstract Art with ML"
      url: "https://janhuenermann.com/blog/abstract-art-with-ml"
      type: "blog"
      category: ""
      lab: "Jan Huenermann"
      description: "I really love the intersection between ML and arts. Here, a process called Computational Pattern Producing networks is used to generate abstract color images from a randomly initialised neural network. The idea seems extremely simple: we give a network some coordinates (x, y) and it outputs 3 values (r, g, b) (or similar for another color-encoding scheme) which we can display as an image. The Computational Pattern Producing pattern appears to be an iterative process whereby the produced image gets refined and sharpened in several steps. The results are quite surprising I have to say. A great source for psychedelic visuals. :) The author uses Tensorflow JS so you can create your own visuals directly in-the-browser. Additional links are included if you want to learn more about Computation Pattern Producing Networks."
      quote: ""
      recommended: ""

    - title: "Why do deep convolutional networks generalize so poorly to small image transformations?"
      url: "https://arxiv.org/abs/1805.12177"
      type: "paper"
      category: "dl"
      lab: "Hebrew University of Jerusalem"
      description: "Authors make the quite shocking observation that modern deep learning architectures are NOT invariant to translation. They explain that this is due to the <i>subsampling</i> operation (generally called <i>stride</i> with CNNs) which is often greater than 1 in modern deep net architectures; besides not being coupled with a pooling operation. As a result, the neural network is not invariant to <i>any</i> translation, but by translations which are multiples of the overall stide. In essence, this means that by shifthing an image by just 1 pixel can result in a completely different network output."
      quote: ""
      recommended: ""

    - title: "Neural scene representation and rendering"
      url: "https://deepmind.com/blog/neural-scene-representation-and-rendering/"
      type: "blog + youtube"
      category: "ml"
      lab: "DeepMind"
      description: "DeepMind keeps bringing some insane stuff. Imagine a 3d-rendered scene (with a ball and a square placed somewhere on the ground) and take two pictures of this scene, from different viewpoints. Give these images to an agent. And now query the agent by asking it <i>to generate the image corresponding to a novel viewpoint</i>. That's exactly what this paper is all about. From a limited number of viewpoints, the agent learns a representation of the scene, such that it is virtually (or <i>mentally</i>) able to move around in the scene. By doing so, it implies the agent has learned relationship between objects (distances, shapes, colors) as well as physical/optical properties of the world (occllusions, illumination, shadows). Highly recommend to check out the article as well as the <a href=https://www.youtube.com/watch?v=G-kWNQJ4idw&feature=youtu.be>presentation video</a>. It's breathtaking!!"
      quote: ""
      recommended: "True"

    - title: "Relation Networks for Object Detection (Oral at CVPR 2018)"
      url: "https://arxiv.org/abs/1711.11575"
      type: "paper"
      category: "cv"
      lab: "Microsoft Research Asia, Peking University"
      description: ""
      quote: "This work proposes an object relation module. It processes a set of objects simultaneously through interaction between their appearance feature and geometry, thus allowing modeling of their relations. It is lightweight and in-place. It does not require additional supervision and is easy to embed in existing networks. It is shown effective on improving object recognition and duplicate removal steps in the modern object detection pipeline. It verifies the efficacy of modeling object relations in CNN based detection. It gives rise to the first fully end-to-end object detector."
      recommended: ""
