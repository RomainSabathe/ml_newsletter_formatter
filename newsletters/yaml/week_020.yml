intro_text:

outro_text:

articles:
    - title: "AI and Compute"
      url: "https://blog.openai.com/ai-and-compute/"
      type: "blog"
      category: "ml"
      lab: "OpenAI"
      description: "A study on the number of operations that were required to train the most famous models of the deep learning era (AlexNet, VGG, LeNet, AlphaZero etc.). It is shown that this number of operations doubles every 3.5 months! In comparison, the Moore's Law estimates that processor's computing power doubles every 18 months. This study was a lot debated and shared last week."
      quote: ""
      recommended: ""

    - title: "ICML 2018 Accepted Papers"
      url: "https://icml.cc/Conferences/2018/AcceptedPapersInitial"
      type: "paper"
      category: "ml"
      lab: ""
      description: ""
      quote: ""
      recommended: ""

    - title: "Automatic Photography with Google Clips"
      url: "https://ai.googleblog.com/2018/05/automatic-photography-with-google-clips.html"
      type: "cv"
      category: "blog"
      lab: "Google AI Blog"
      description: "Remember! 17 weeks ago (!) in this very same newsletter we were telling you about <a href=https://design.google/library/ux-ai/>The UX of AI</a>. A blog post that showed Google's interest in trying to identify <i>worthy moments</i> in your videos. Your kids are in the parc, one of them does a backflip. That's what you want the algorithm to curate for you! At the time we didn't have much information regarding the training procedure. Now this article brings us a lot more information. What I thought was the most stunning is the HUGE amount of human labelling involved. Instead of giving a score to video samples (say 5 is 'super worthy' and 0 is 'not worthy at all'), researchers decided it would be wiser to give labellers two video samples. All the labellers had to do was to say which of the two videos is the worthier. And they obtained 50,000,000 of these comparisons. Yes. <b>50 MILLION manually labelled pairs of video samples</b>. Another interesting bit is how they tackled the problem of fairness. You can tell they cared a lot about this. For now what they did was to insure the training data was as diverse as it can and similarly the testing data featured variations of sensible features (skin color, age, etc.) under static global conditions (background, action etc.)."
      quote: ""
      recommended: ""

    - title: "Deep Video Portraits"
      url: "https://www.youtube.com/watch?v=qc5P2bvfl44"
      type: "video"
      category: "cv"
      lab: "H. Kim et al."
      description: "We have all seen these videos where by filming myself open my mouth, I could make a celebrity (like Barack Obama) opens his mouth in exactly the same way. Well this work is related to this task. One could even say it does the same thing. Although I have to say here I was stunned! The video results are astonishing. It's only once the video is scaled up that we can start detecting artifacts... Worth giving it a watch.."
      quote: ""
      recommended: ""

    - title: "Best Practices for ML Engineering"
      url: "https://developers.google.com/machine-learning/rules-of-ml/"
      type: "blog"
      category: "ml"
      lab: "Google"
      description: "Didn't properly read it all... But it's just because I want to properly dig into that!! I feel like this page is a mine of tips and advises to keep in mind when designing a machine learning <i>product</i> from A to Z. In particular, I was suprised by the very first rule: Google, which consider itself as a 'AI first' company, recommends not to be afraid to launch a product without machine learning. Plenty of other of these recommendations suggest that good engineering is at least as important as being a good ML practioner."
      quote: ""
      recommended: "true"

    - title: "Where are internet cats gone?"
      url: "https://twitter.com/ajmooch/status/998207618349977600"
      type: "twitter"
      category: "laugh"
      lab: ""
      description: ""
      quote: ""
      recommended: ""

    - title: "Delayed Impact of Fair Machine Learning"
      url: "http://bair.berkeley.edu/blog/2018/05/17/delayed-impact/"
      type: "blog"
      category: "fairness"
      lab: "Berkely AI Research"
      description: "This post makes use of a simple credit scoring scenario and a few visualisations to make us understand the difficulty (and even danger!) of choosing a fairness criterion. We are invited to take the role of a bank which can apply two different thresholds on the credit score for two different populations (one of them being a 'minority'). We see how intuitive fairness goals such as <i>demographic parity</i> or <i>equal opportunity</i> can actually be detrimental to the long-term well-being of the minority population! This work prones to have a broader view of 'fairness'; one which optimises the future of a population."
      quote: ""
      recommended: "true"

    - title: "Navigating with grid-like representations in artificial agents"
      url: "https://deepmind.com/blog/grid-cells/"
      type: "blog + paper"
      category: "rl"
      lab: "DeepMind"
      description: "A short presentation of the newest Nature paper from DeepMind. I thought this work was interesting particularly because it shows such strong connection between artificial models and biological models of the brain. In the brain, grid-organised type of cells were shown to be associated with spacial localisation. By using RNNs for the task of self-localisation of a reinforcement learning agent, DeepMind researchers showed that grid-like patterns were naturally emerging in the artificial network. They monitored the activity to these units and observed indeed strong correlation with the localisation of the agent. Also, the agent was performing better at the task of navigation. This also opens up the possibility of using artificial networks to experiment and predict the behaviour or biological models. As said in the post, this has the potential to complement works already conducted on amimals."
      quote: ""
      recommended: ""

    - title: "3D Printed Robot Cat learns to walk with Machine Learning"
      url: "https://www.youtube.com/watch?v=zNXgT2csQ7A"
      type: "video"
      category: "rl"
      lab: "University of Stavanger"
      description: "In less than 2 hours, a small quadruped learns to walk efficiently by only using genetic algorithms. And it's not in a simulation! Another example that GA is competitive compared to standard gradient descent methods for reinforcement learning tasks."
      quote: ""
      recommended: ""

    - title: "Small steps and giant leaps: Minimal Newton solvers for Deep Learning"
      url: "https://arxiv.org/abs/1805.08095"
      type: "paper"
      category: "optim"
      lab: "University of Oxford"
      description: "When Vedaldi is listed as one of the authors, it makes you want to look at this paper more carefully. Really interested in testing this algorithm and implementing it."
      quote: "Our method addresses long-standing issues with current second-order solvers, which invert an approximate Hessian matrix every iteration exactly or by conjugate-gradient methods, a procedure that is both costly and sensitive to noise. Instead, we propose to keep a single estimate of the gradient projected by the inverse Hessian matrix, and update it once per iteration. [...] We then train several large models on CIFAR and ImageNet, including ResNet and VGG-f networks, where we demonstrate faster convergence with no hyperparameter tuning. Code is available."
      recommended: ""

    - title: "Hierarchical Neural Story Generation"
      url: "https://arxiv.org/abs/1805.04833v1"
      type: "paper"
      category: "nlp"
      lab: "Facebook AI Research"
      description: "If you have two minutes, go and check the last page of the paper. The goal is to generate stories; something that has been explored many times in the past (sometimes reaching <a href=https://www.theverge.com/2016/1/21/10805398/friends-neural-network-scripts>headlines</a>). Here I was very impressed by the overall quality of the generated text. It still feels like empty somehow, but it also seems coherent throughout the entire paragraph. This is the whole selling point of the paper: by using a hierarchical generation procedure, authors were able to generate texts that had a strong narrative baseline."
      quote: ""
      recommended: ""

    - title: "Gaussian Material Synthesis"
      url: "https://www.youtube.com/watch?v=cnquEovq1I4"
      type: "paper + video"
      category: "cv"
      lab: "TU Wien"
      description: "This work has received quite a bit of attention, both because it is interesting, but also because its first author is Károly Zsolnai-Fehér, the man behind the series of videos 'Two minute papers'. The idea is to use a neural network to perform material synthesis. The network can learn from the user's preferences, suggest new materials and importantly generate an image in a few miliseconds (compared to 40-60 seconds with traditional methods)."
      quote: ""
      recommended: ""

    - title: "A Universal Music Translation Network"
      url: "https://www.youtube.com/watch?v=vdxCqNWTpUs"
      type: "video + paper"
      category: ""
      lab: "Facebook AI Research"
      description: "I am very impressed by what they achieved. They transfer (to some extent) musics from one genre to the other. Unlike similar works that limited only to MIDI data (which doesn't sound great let's be honest), here the waveforms (or frequencies?) are generated directly. This introduces problems of artifacts that we can clearly hear, but overall the pace and dynamic of a piece are well transfered into another genre."
      quote: ""
      recommended: ""

    - title: "Self-Attention Generative Adversarial Networks"
      url: "https://arxiv.org/abs/1805.08318"
      type: "paper"
      category: "gan"
      lab: "Rutgers University, Google Brain"
      description: "It's still a preprint, but the idea is interesting and the authors (which includes Ian Goodfellow) mention what looks like to be considerable improvement when generating images based on ImageNet. It seems two main ideas are at work: one is to sample the latent space multiple times and potentially at different regions; the second is to provide the discriminator with attention capabilities: it will be able to check the consistency of a generated sample by comparing distant portions of the image."
      quote: ""
      recommended: ""
