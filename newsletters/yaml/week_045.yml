intro_text: ""

outro_text:

articles:
- title: "The Illustrated BERT"
  lab: "Jay Alammar"
  url: "https://jalammar.github.io/illustrated-bert/"
  type: "blog"
  category: "nlp"
  description: "We already talked about <a href=https://github.com/google-research/bert>BERT</a> in a previous newsletter. This is a new method proposed by Google to pretrain language representations. It went popular very quickly as authors proved that one can achieve SOTA results on a number of different NLP-related tasks by finetuning the representations obtained with BERT. Similarly to what we would do in computer vision by finetuning a model trained on ImageNet (see a related article <a href=http://ruder.io/nlp-imagenet/>here</a> by Sebastian Ruder). If you want to learn more about BERT, this blog post could help you! All the concepts are explained in a visual-friendly approach."

- title: "The deepest problem with deep learning"
  lab: "Gary Marcus"
  type: "medium"
  category: "other"
  url: "https://medium.com/@GaryMarcus/the-deepest-problem-with-deep-learning-91c5991f5695"
  description: "This post is a bit of a long read. Gary Marcus who has long been pictured as a <i>symbolist</i> explains why he thinks deep learning can't solve artificial intelligence by itself. Instead, he mentions <i>a hybrid model that vastly outperformed what a purely deep net would have done, incorporating both back-propagation and a (continuous versions) of the primitives of symbol-manipulation</i>. This post follows a <a href=https://twitter.com/GaryMarcus/status/1065280340669816832?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1065280340669816832&ref_url=https%3A%2F%2Fmedium.com%2Fmedia%2F45e2aca1fc1bb180d310c6e739d392ed%3FpostId%3D91c5991f5695>debate</a> that has happened on Twitter a few days ago."

- title: "When a GAN draws Garfield"
  url: "https://trello-attachments.s3.amazonaws.com/5bf506f9456de85b3a3d0024/5c08c92f2edc43816e1cc49d/0b0e6335789cdcdd43b088f4917f0f7a/p3CAHLA.jpg"
  category: "laugh"
  lab: "vdalv"
  type: "blog"
  description: "More seriously, article <a href=https://vdalv.github.io/2018/12/04/ganfield.html>here.</a>"

- title: "arXiv - Bag of Tricks for Image Classification with Convolutional Neural Networks"
  lab: "Amazon Web Services"
  type: "paper"
  category: "cv"
  url: "https://arxiv.org/abs/1812.01187"
  quote: "[...] In this paper, we will examine a collection of [traditional] refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3% to 79.29% on ImageNet. [...]"

- title: "Lambdo: feature engineering and machine learning together"
  url: https://github.com/asavinov/lambdo
  lab: "Alexandr Savinov"
  category: "ds"
  type: "github"
  quote: "Lambdo is a workflow engine which significantly simplifies the analysis process by unifying feature engineering and machine learning operations. Lambdo data analysis workflow does not distinguish between them and any node can be treated either as a feature or as prediction, and both of them can be trained."

- title: "arXiv - Second-order Optimization Method for Large Mini-batch: Training ResNet-50 on ImageNet in 35 Epochs"
  url: "https://arxiv.org/abs/1811.12019"
  type: "paper"
  category: "theory"
  lab: "Tokyo Institute of Technology, NVIDIA, RIKEN Center for Computational Science"
  quote: "[...] We propose an alternative approach using a second-order optimization method that shows similar generalization capability to first-order methods, but converges faster and can handle larger mini-batches. To test our method on a benchmark where highly optimized first-order methods are available as references, we train ResNet-50 on ImageNet. We converged to 75% Top-1 validation accuracy in 35 epochs for mini-batch sizes under 16,384, and achieved 75% even with a mini-batch size of 131,072, which took 100 epochs."

- title: "MLPerf results are available"
  url: "https://mlperf.org/results/"
  lab: "MLPerf"
  type: "github"
  category: "engineering"
  description: "Back in May the <a href=https://mlperf.org/>MLPerf</a> project was created (we talked about it in the 17th issue of this newsletter!) with the goal of finally coordinating the benchmark of hardware setups for machine learning. In other words, a list of precise problems and precise constraints were imposed and several architectures (TPUs, DGXs,..) were used and ranked against each other. Plenty of companies and universities have participated. And now we have the results! NVIDIA was super proud to show the <a href=https://news.developer.nvidia.com/nvidia-captures-top-spots-on-mlperf/>speed ups they obtained</a>."

- title: "An introduction to probabilistic programming, now available in TensorFlow Probability"
  lab: "Tensorflow Probability, Google"
  type: "medium"
  category: "ml"
  url: "https://medium.com/tensorflow/an-introduction-to-probabilistic-programming-now-available-in-tensorflow-probability-6dcc003ca29e?linkId=60908456"
  description: "Tensorflow Probability has been released earlier this year. It provides the traditional set of probabilistic programming tools inside the Tensorflow framework. This blog post is a tiny introduction of what TFP can do. More precisely, in this example authors estimate the probability that a certain component used on the Challenger space shuttle (1986) fails under certain temperatures. More generally, the post is here to announce that the very famous <a href=https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers>Programming and Bayesian Methods for Hackers</a> tutorial has been extended to include examples written using Tensorflow Probability. I'm also mentioning that the next major release of PyMC, one of the most famous probabilistic programming package for Python, will be using <a href=https://medium.com/@pymc_devs/theano-tensorflow-and-the-future-of-pymc-6c9987bb19d5>Tensorflow Probability</a> as its backend instead of Theano."

- title: "A Style-Based Generator Architecture for Generative Adversarial Networks"
  lab: "NVIDIA"
  type: "paper + video"
  category: "dl"
  url: "https://arxiv.org/abs/1812.04948"
  description: "I'm sure you remember the <a href=https://arxiv.org/abs/1710.10196>Progressive Growing of GANs</a> paper. In <i>A Style-Based Generator Architecture for GANs</i>, authors managed to generate images with the same amazing level of realism as they have done in Progressive Growing, but now the model also learns a disentangled representation at the same time. Authors have released a <a href=https://www.youtube.com/watch?v=kSLJriaOumA&feature=youtu.be>video</a> showing more visual results of their method."
  quote: "We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes [...] and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. [...]"
