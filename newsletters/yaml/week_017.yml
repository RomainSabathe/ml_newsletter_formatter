intro_text:

outro_text:

articles:
    - title: "All Google publications at ICLR 2018"
      url: "https://research.googleblog.com/2018/04/google-at-iclr-2018.html"
      type: "blog"
      category: "general"
      lab: "Google"
      description: "There's no best way to experience how much Google publishes. <a href=https://deepmind.com/blog/deepmind-papers-iclr-2018/>DeepMind has done the same thing</a>."
      quote: ""
      recommended: ""

    - title: "Einsum is all you need - Einstein summation in Deep Learning"
      url: "https://rockt.github.io/2018/04/30/einsum"
      type: "blog"
      category: ""
      lab: "Tim Rocktaschel"
      description: "Did you know that the Einstein summation was supported in Numpy, Tensorflow and Pytorch? This blog post explains how to use it and how it can make complex matrix multiplications super easy; especially when it involves convoluted shapes and batch dimensions. Tons of examples are provided. Some people has been questionning the speed efficiency of <i>einsum</i> functions though."
      quote: ""
      recommended: "true"

    - title: "Facebook open sources ELF OpenGo"
      url: "https://research.fb.com/facebook-open-sources-elf-opengo/"
      type: "blog"
      category: "rl"
      lab: "Facebook Research"
      description: "Facebook reproduced the results from AlphaGoZero and unlike DeepMind released the code as well as a pretained model. A single GPU is apparently enough to run the model in real-time. It's also a good operation to promote Facebook's reinforcement learning library called ELF."
      quote: ""
      recommended: ""

    - title: "The road to 1.0: production ready PyTorch"
      url: "https://pytorch.org/2018/05/02/road-to-1.0.html"
      type: "blog"
      category: "news"
      lab: "Facebook Research"
      description: "Very exciting news for PyTorch. Engineers are targeting to release PyTorch 1.0 this summer. It will be 'production-ready'. This doesn't only mean that the library needs to be stable, it also needs to be scalable, low-level optimised and deployable on several architectures. The post explains the design decisions that have been taken to get there. I'm pleased to see engineers are doing their best to offer both the best of the flexibility of PyTorch and the rigidity of an optimised ML code. TO this end they will release a Just In Time compiler. It will dynamically analyse the Python code and generate and underlying C++ optimised code for faster computation."
      quote: ""
      recommended: "true"

    - title: "Advancing state-of-the-art image recognition with deep learning on hashtags"
      url: "https://code.facebook.com/posts/1700437286678763/"
      type: "paper + blog"
      category: "cv"
      lab: "Facebook Research"
      description: "The claim is bold here. +2% accuracy on ImageNet thanks to 'semi-supervised' learning. The idea is to extend the training data ny using public online images with their hastag annotations."
      quote: ""
      recommended: ""

    - title: "Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network"
      url: "https://github.com/YadiraF/PRNet"
      type: "github + paper"
      category: "cv"
      lab: "YadiraF"
      description: "Better keep an eye on that repo. More than 600 stars. It provides an end-to-end model to regress the 3D geometry of a face and its semantic from a 2D image. They use their method and extend it to perform face alignment, 3D face reconstruction and pose estimation. It is claimed to be robust to poses, illuminations and occlusions. Oh, and it runs at 100fps (on a 1080...)."
      quote: ""
      recommended: "true"

    - title: "The loss landscape of overparameterized neural networks"
      url: "https://arxiv.org/abs/1804.10200v1"
      type: "paper"
      category: "theory"
      lab: "Y. Cooper"
      description: "Remember last week we linked a work from Uber where they explored a so-called 'Intrinsic Dimension' of neural networks? These were empirical observations stating that a good minima of the loss function lied in a subspace of lower dimension. If you have 1000 parameters (therefore the loss function is 1000-dimentional), it could well be that that a minima can be found by solely exploring a subspace of only 200 dimensions. Although I haven't read the paper linked here, I feel like the abstract describes exactly this observation. The author attempts to provide a theoretical explanation."
      quote: ""
      recommended: ""

    - title: "Training Imagenet in 3 hours for $25; and CIFAR10 for $0.26"
      url: "http://www.fast.ai/2018/04/30/dawnbench-fastai/"
      type: "blog"
      category: "ml"
      lab: "fast.ai"
      description: "A student from fast.ai achieved training a model on ImageNet up to 93% accuracy in a very short amount of time and at a reduced cost (notebook <a href=https://github.com/radekosmulski/machine_learning_notebooks/blob/master/cifar10_fastai_dawnbench.ipynb>here</a>). This blog post explains the method they followed. It seems they relied on the phenomenon of <i>super convergence</i> apparently discovered last year where the learning rate is gradually increased while the momentum is gradually lowered. They used half-precision floats as well as a technique inspired by <i>Progressive Growing of GANs</i> (discussed by Joao in our paper reading group). They started training the network on small images and increased the size of the input images along training. To make this possible they made use of 'adaptative pooling'. <br />The whole blog post is also a lampoon against the race to more computing power and more data. Jeremy Howard (founder of fast.ai and author of the post) obviously promotes the creativity and awesomeness of his class.... "
      quote: "I worry when I talk to my friends at Google, OpenAI, and other well-funded institutions that their easy access to massive resources is stifling their creativity."
      recommended: ""

    - title: "Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models"
      url: "https://arxiv.org/abs/1711.05772"
      type: "paper"
      category: "gan"
      lab: "Google Brain"
      description: ""
      quote: "Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation enables interactive control, but creating new controls often requires expensive retraining. In this paper, we develop a method to condition generation without retraining the model. By post-hoc learning latent constraints, value functions that identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions with gradient-based optimization or amortized actor functions."
      recommended: ""

    - title: "List of Machine Learning Contests"
      url: "https://github.com/skrish13/ml-contests-conf"
      type: "github"
      category: ""
      lab: "skrish13"
      description: "All is said."
      quote: ""
      recommended: ""

    - title: "Tutorial and demo and private ML thanks to distributed training"
      url: "https://www.youtube.com/watch?v=iYP4sYz0jho&feature=youtu.be&t=1h13m21s"
      type: "youtube"
      category: "fairness"
      lab: "OpenMined"
      description: "Could be useful for us in the future."
      quote: ""
      recommended: ""

    - title: "MLPerf"
      url: "https://mlperf.org/"
      type: ""
      category: ""
      lab: ""
      description: "A suite of tools to perform benchmarking of ML framework, cloud platforms and hardware accelerators. This initiative is backed up by some big names (Google, Intel, AMD (!), Baidu, Stanford, Berkeley and others). The deadline is set to the end of July so we'll have some juicy comparisons in a few months."
      quote: ""
      recommended: ""

    - title: "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review"
      url: "https://arxiv.org/abs/1805.00909"
      type: "paper"
      category: "rl"
      lab: "UC Berkeley"
      description: ""
      quote: ". In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics."
      recommended: ""
