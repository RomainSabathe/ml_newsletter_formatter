articles:
  - category: dl
    description: 'Open AI used the Sparse Transformer network presented last week
      to generate 4-minutes-long music clips with high temporal consistency. We also
      learn that they used a clever combination of different embeddings (by encoding
      notes in chord, or the passage of time, etc) and trained the model on different
      MIDI datasets. <br /> They have been running a musical competition all week
      long with <a href=https://twitter.com/OpenAI/status/1123048808013750272>announced
      winners</a> everyday.'
    lab: OpenAI
    title: MuseNet
    type: blog
    url: https://openai.com/blog/musenet/
  - category: engineering
    description: 'Pytorch 1.1 has been released and it comes with some great additions.
      My selection: TensorBoard is officially added although at an experimental stage,
      a few additions to the JIT compiler, new possibilities for model parallelism,
      some nice additions to the nn library (like the MultiheaderAttention module),
      new learning rate schedulers (cyclical learning rate and cosine annealing with
      warm restarts), some impressive speedups on CPU (batch norm at inference is
      reported to be ~19 times faster!!)'
    lab: pytorch
    title: 'Pytorch 1.1'
    type: github
    url: https://github.com/pytorch/pytorch/releases/tag/v1.1.0
  - category: vis
    description: 'Choose a suitable alpha and use f(x) = sin2(2^(xt) arcsin(sqrt(alpha)).
      That''s all you need to fit a dataset perfectly. The author provides a document
      explaining how to tune the alpha parameter and how this is possible. Of course,
      don''t expect any generalization capabilities.'
    lab: 'Laurent Bou√©'
    title: 'Real numbers, data science and chaos: How to fit any dataset with a single
      parameter'
    type: paper
    url: https://github.com/Ranlot/single-parameter-fit
  - category: rl
    description:
    lab: DeepMind
    quote: 'Rather than proposing a new method, this paper investigates an issue present
      in existing learning algorithms. We study the learning dynamics of reinforcement
      learning (RL), specifically a characteristic coupling between learning and data
      generation that arises because RL agents control their future data distribution.
      In the presence of function approximation, this coupling can lead to a problematic
      type of ''ray interference'', characterized by learning dynamics that sequentially
      traverse a number of performance plateaus, effectively constraining the agent
      to learn one thing at a time even when learning in parallel is better. <b>We
      establish the conditions under which ray interference occurs, show its relation
      to saddle points and obtain the exact learning dynamics in a restricted setting.
      We characterize a number of its properties and discuss possible remedies.</b>'
    title: "[arXiv] Ray Interference: a Source of Plateaus in Deep Reinforcement Learning"
    type: paper
    url: https://arxiv.org/abs/1904.11455
  - category: cv
    description:
    lab: 'NVIDIA, University of Toronto, Vector Institute, MIT'
    quote: "[...] We propose Meta-Sim, which learns a generative model of synthetic\
      \ scenes, and obtain images as well as its corresponding ground-truth via a\
      \ graphics engine. <b>We parametrize our dataset generator with a neural network,\
      \ which learns to modify attributes of scene graphs obtained from probabilistic\
      \ scene grammars, so as to minimize the distribution gap between its rendered\
      \ outputs and target data.</b> If the real dataset comes with a small labeled\
      \ validation set, we additionally aim to optimize a meta-objective, i.e. downstream\
      \ task performance. Experiments show that the proposed method can greatly improve\
      \ content generation quality over a human-engineered probabilistic scene grammar,\
      \ both qualitatively and quantitatively as measured by performance on a downstream\
      \ task."
    title: "[arXiv] Meta Sim: Learning to Generate Synthetic Datasets"
    type: paper
    url: https://nv-tlabs.github.io/meta-sim/
  - category: dl
    description:
    lab: 'Google Brain, Carnegie Mellon University'
    quote: "[...] In this work, we propose to apply data augmentation to unlabeled\
      \ data in a semi-supervised learning setting. <b>Our method, named Unsupervised\
      \ Data Augmentation or UDA, encourages the model predictions to be consistent\
      \ between an unlabeled example and an augmented unlabeled example.</b> Unlike\
      \ previous methods that use random noise such as Gaussian noise or dropout noise,\
      \ UDA has a small twist in that it makes use of harder and more realistic noise\
      \ generated by state-of-the-art data augmentation methods. [...] . <b>On the\
      \ IMDb text classification dataset, with only 20 labeled examples, UDA outperforms\
      \ the state-of-the-art model trained on 25,000 labeled examples</b>. On standard\
      \ semi-supervised learning benchmarks, CIFAR-10 with 4,000 examples and SVHN\
      \ with 1,000 examples, UDA outperforms all previous approaches and reduces more\
      \ than 30% of the error rates of state-of-the-art methods [...]."
    title: "[arXiv] Unsupervised Data Augmentation"
    type: paper
    url: https://arxiv.org/abs/1904.12848
  - category: gan
    description:
    lab: 'Google Brain, ETH Zurich'
    quote: "[...] In this work we demonstrate how one can benefit from recent work\
      \ on self- and semi-supervised learning to outperform state-of-the-art (SOTA)\
      \ on both unsupervised ImageNet synthesis, as well as in the conditional setting.\
      \ In particular, <b>the proposed approach is able to match the sample quality\
      \ (as measured by FID) of the current state-of-the art conditional model BigGAN\
      \ on ImageNet using only 10% of the labels</b> and outperform it using 20% of\
      \ the labels."
    title: "[ICML 2019] High-Fidelity Image Generation With Fewer Labels"
    type: paper
    url: https://arxiv.org/abs/1903.02271
  - category: dl
    description:
    lab: 'Google Brain'
    quote: "[...] In this paper, we consider the use of self-attention for discriminative\
      \ visual tasks as an alternative to convolutions. We introduce a novel two-dimensional\
      \ relative self-attention mechanism that proves competitive in replacing convolutions\
      \ as a stand-alone computational primitive for image classification. [...] <b>We\
      \ therefore propose to augment convolutional operators with this self-attention\
      \ mechanism by concatenating convolutional feature maps with a set of feature\
      \ maps produced via self-attention.</b> Extensive experiments show that Attention\
      \ Augmentation leads to consistent improvements in image classification on ImageNet\
      \ and object detection on COCO across many different models and scales, including\
      \ ResNets and a state-of-the art mobile constrained network, while keeping the\
      \ number of parameters similar. [...]"
    title: "[arXiv] Attention Augmented Convolutional Networks"
    type: paper
    url: https://arxiv.org/abs/1904.09925
intro_text: ""
outro_text: ""
