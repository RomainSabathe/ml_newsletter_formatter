intro_text:

outro_text:

articles:
    - title: "ML beyond Curve Fitting: An Intro to Causal Inference and do-Calculus"
      url: "http://www.inference.vc/untitled/"
      type: "blog"
      category: "optim"
      lab: "inFERENce"
      description: "Last week, we mentioned an interview of Judea Pearl in which he described machine learning as 'curve fitting'. He put an emphasis on causality as a crucial step in AI research. In this bog post, the author admittedly overlooked this concept in the past. But not anymore! He explains in a simple scenario why and how it could change the way we think about data distributions."
      quote: ""
      recommended: ""

    - title: "3D Face Reconstruction with Position Map Regression Networks"
      url: "https://heartbeat.fritz.ai/3d-face-reconstruction-with-position-map-regression-networks-36f0ac2d3ef1"
      type: "blog"
      category: "cv"
      lab: "Favio Vazquez"
      description: "A short but informative description of the algorithm described in the <a href=https://arxiv.org/abs/1803.07835><i>Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network</i></a> paper. We talked about it (and its GitHub repo) a few weeks ago (week 17). It still looks incredible..."
      quote: ""
      recommended: ""

    - title: "Entropy and mutual information in models of deep neural networks"
      url: "https://arxiv.org/abs/1805.09785"
      type: "paper"
      category: "optim"
      lab: "Ecole Normale Superieure de Paris, INRIA Saclay, CNRS, Ecole Polytechnique Federale de Lausanne, Sorbonne Universites "
      description: "These are all outstanding labs. And it's more than 60 pages long."
      quote: "We examine a class of deep learning models with a tractable method to compute information-theoretic quantities."
      recommended: ""

    - title: "Why you need to improve your training data, and how to do it"
      url: "https://petewarden.com/2018/05/28/why-you-need-to-improve-your-training-data-and-how-to-do-it/"
      type: "blog"
      category: "ml"
      lab: "Pete Warden"
      description: "A good reminder that the quickest way to increase a model's performance is by curating its training data and increasing the volume of training data. I didn't find anything ground-breaking in this post. Yes, it's a good reminder to QC the data manually, to try to maximise the type of data we collect and that it is crucial to optimise the tools that are used to generate the labels (he quotes the anecdote where images with label 'Jaguar' were picturing both jaguar cats and cars. By improving the labelling tools, they managed to avoid this sort of mistake)."
      quote: ""
      recommended: ""

    - title: "Towards a Theoretical Understanding of Batch Normalization"
      url: "https://arxiv.org/abs/1805.10694"
      type: "paper"
      category: "theory"
      lab: "ETH Zurich, University of Rostock"
      description: "Really want to look into that one! Note that the MIT released a paper on the same subject this week!! <a href=https://arxiv.org/abs/1805.11604> here.</a>."
      quote: "We thereby turn bach normalization from an effective practical heuristic into a provably
      converging algorithm. Additionally we show that this technique can be considered as a nonlinear
      reparametrization of the weight space, which can be very beneficial even for simple convex
      optimization tasks, such as logistic regressions."
      recommended: ""

    - title: "Empricism and limits of gradient descent"
      url: "http://togelius.blogspot.com/2018/05/empiricism-and-limits-of-gradient.html"
      type: "blog"
      category: "theory"
      lab: "Togelius"
      description: "This post praises the use of genetic algorithms (with or without gradient descent). The author partly relies on philosophy to make his point, which I found all the more interesting. In epistemology, he claims, there are at least two ways of understanding how we gather knowledge. First is Locke's view where our mind is like a blank slate and each observation writes some information onto our blank slate. This would be equivalent to SGD: every time we get an error (some new information), we take it into account to modify the slate of our mind. Opposed to this is the <i>critical rationalism</i> from Karl Popper. Here, we gather knowledge by making hypotheses and we confront our hypotheses by observing the world. According to the author, this is more similar to genetic algorithm since each child holds a different hypothesis and they are all in competition for the most likely hypothesis. He also mentions the difference between <i>critical rationalism</i> and <i>logical empirism</i>. Interesting read, especially if you like philosophy. But don't expect to find formulas or real proofs."
      quote: ""
      recommended: ""

    - title: "RL Adventure"
      url: "https://github.com/higgsfield/RL-Adventure-2"
      type: "github"
      category: "rl"
      lab: "higgsfield"
      description: "Tutorials on a ton of different RL algorithms. Seems like a great ressource to start from. For each algorithm, the author provides the relevant paper, a Jupyter notebook implementation and sometimes a blog post that covers the algorithm. Super nice!!"
      quote: ""
      recommended: ""

    - title: "Image Colorization with Convolutional Neural Networks"
      url: "https://lukemelas.github.io/image-colorization.html"
      type: "blog"
      category: "cv"
      lab: "Luke Melas"
      description: "A small project yet effective. Colorization is interpreted as a regression problem where, starting for the lightness information, the model should output the hue and saturation (in the Lab colorspace)."
      quote: ""
      recommended: ""

    - title: "Yellowbrick"
      url: "https://github.com/DistrictDataLabs/yellowbrick"
      type: "github"
      category: "vis"
      lab: "DistrictDataLabs"
      description: "Another data visualisation library... This one is different in the sense that it is explicitely designed to explore datasets and facilitate model selection. It does not only provide plotting functions, but also statistical algorithms. It uses the same API as sklearn (model.fit(), etc.). Although probably targeted to data sciency applications, it looks like a complete good-to-have library."
      quote: ""
      recommended: ""

    - title: "Gym Retro"
      url: "https://blog.openai.com/gym-retro/"
      type: "blog"
      category: "rl"
      lab: "OpenAI"
      description: "The Gym environment is a super neat Python framework to emulate games and scenarios. It provides the frames in real time as well as the key information from the environment (for instance: the distance your character has travelled, how many lives you still  have etc.). There were already about a hundred retro games available in Gym, but OpenAI just added a few new games... So now they are a thousand...!!"
      quote: ""
      recommended: ""

    - title: "Hyperbolic Attention Networks"
      url: "https://arxiv.org/abs/1805.09786"
      type: "paper"
      category: "dl"
      lab: "DeepMind"
      description: "For hierarchical structures, the common way-to-go is to model the situation with a tree of relationships (parent-children) and to think based on the tree. In certain type of trees, the number of nodes (i.e. of concepts) grows exponentially with the depth of the tree, which gives us an indication on the type of geometry that link the nodes together: some sort of exponential distance between concepts. That is what this paper explores by looking at hyperbolic spaces. In hyperbolic spaces, the area of a circle grows exponentially with the radius (whereas the growth is polynomial in a Euclidean space). This property is used to model an exponential distance between concepts (as given in the paper: Pug-Dog-Mammal) and is therefore assumed to better represent the structure imposed by a tree.  "
      quote: ""
      recommended: ""

    - title: "1st place solution in the Google Landmark Retrieval Challenge"
      url: "https://www.kaggle.com/c/landmark-retrieval-challenge/discussion/57855"
      type: "blog"
      category: "ml"
      lab: "anokas"
      description: "In this challenge, you are given an image or a landmark and the goal is to retrieve all the images of this landmark in a dataset. As it is the rule for Kaggle competitions, there is heavy use of ensembles but I was also surprised by the diversity of techniques involved. In particular, at least 4 different schemes are used to extract image features from the output of a pre-trained CNN. They also used a variant of ResNet called ResNetXt."
      quote: ""
      recommended: ""
