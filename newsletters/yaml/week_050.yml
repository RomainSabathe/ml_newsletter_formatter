articles:
  - category: theory
    description: 'That is a really interesting blog post in which the author explores
      the relationship between batch norm and L2 regularization. In the first part,
      we see how L2 regularization is virtually useless when using batch normalization
      (because batch norm is invariant to the scale of weights). However, in the second
      part, the author describes how L2 reg can change the magnitude of the gradient
      updates relative to the norm of the weights. In essence, using batch norm without
      L2 regularization acts as if the learning rate was decaying during training.
      The justification for this phenomenon is first given using simple formulas.
      Then, experiments are conducted on ResNet-20 and CIFAR-10 to verify the theory.
      The graphs shown near the end of the post are quite insightful.'
    lab: 'David Wu'
    recommended: true
    title: 'L2 Regularization and Batch Norm'
    type: blog
    url: https://blog.janestreet.com/l2-regularization-and-batch-norm/
  - category: dl
    description:
    lab: 'University of Oxford, Alan Turing Institute'
    quote: 'We introduce semi-unsupervised learning, an extreme case of semi-supervised
      learning with ultra-sparse categorisation where some classes have no labels
      in the training set. That is, in the training data some classes are sparsely
      labelled and other classes appear only as unlabelled data. [...] We develop
      two deep generative models for classification in this regime[..]. By changing
      their probabilistic structure to contain a mixture of Gaussians in their continuous
      latent space, these new models can learn in both unsupervised and semi-unsupervised
      paradigms. [...]'
    title: "[arXiv] Semi-Unsupervised Learning with Deep Generative Models: Clustering\
      \ and Classifying using Ultra-Sparse Labels"
    type: paper
    url: https://arxiv.org/abs/1901.08560v1
  - category: ml
    description: 'This GitHub page offers a substantial list of famous projects  in
      one place (Pix2Pix, eager execution, Sketcher, BigGan, U-Net, Mask R-CNN and
      others,...). What''s more, all the implementations are available through a Colab
      notebook for immediate experimentation. :-)'
    lab: 'Zaid Alyafeai'
    title: 'Machine learning notebooks in different subjects optimized to run in google
      collaboratory'
    type: github
    url: https://github.com/zaidalyafeai/Notebooks
  - category: dl
    description: 'Feature visualisation for neural networks is not something particularly
      new. To me, the reference article remains this <a href=https://distill.pub/2017/feature-visualization/>Distill
      article</a> by Chris Olah et al. But what I liked with this post is that the
      feature visualisations are harder to analyse compared to the ones shown in the
      Distill article. The author does a good job at selecting a few features for
      which we can only have an vague intuition of what they are responsible for.
      To verify these intuitions, the author selects a few images which may match
      the said filter and analyse the network''s response.'
    lab: 'Fabio M. Graetz'
    title: 'How to visualize convolutional features in 40 lines of code'
    type: blog
    url: https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030
  - category: rl
    description: 'Now that more and more labs managed to solve Montezuma''s Revenge,
      it seems the RL community is looking for a next challenge in vision, motion
      and exploration. This tool provided by Unity might be the one. The world is
      in 3D and procedurally generated with quite a variety of environments. It''s
      also directly compatible with Gym for faster integration. I''m eager to see
      future results on this benchmark.'
    lab: 'Unity Technologies'
    quote: 'The Obstacle Tower is a procedurally generated environment consisting
      of multiple floors to be solved by a learning agent. It is designed to test
      learning agents abilities in computer vision, locomotion skills, high-level
      planning, and generalization. [...]'
    title: 'Obstacle Tower Environment'
    type: github
    url: https://github.com/Unity-Technologies/obstacle-tower-env
  - category: dl
    description: 'This new dataset looks super neat. It comes with 20 million questions
      on a variety of images. Each image is annotated with spatial relationships (like:
      the glass is on the table), plus coordinates of the objects in the scene. All
      the questions are also categorised by semantic and structural types. The website
      offers nice <a href=https://cs.stanford.edu/people/dorarad/gqa/vis.html>visuals</a>
      to quickly explore what the dataset has to offer.'
    lab: 'Drew Hudson & Christopher Manning'
    quote: 'A New Dataset for Visual Question Answering'
    title: 'Dataset: Visual Reasoning in the Real World'
    type: paper
    url: https://cs.stanford.edu/people/dorarad/gqa/
  - category: theory
    description:
    lab: 'Google AI'
    quote: 'We investigate neural network training and generalization using the concept
      of stiffness. We measure how stiff a network is by looking at how a small gradient
      step on one example affects the loss on another example. [...] Our results demonstrate
      that stiffness is a useful concept for diagnosing and characterizing generalization.
      We observe that small learning rates lead to initial learning of more specific
      features that do not translate well to improvements on inputs from all classes,
      whereas high learning rates initially benefit all classes at once. We measure
      stiffness as a function of distance between data points and observe that higher
      learning rates induce positive correlation between changes in loss further apart,
      pointing towards a regularization effect of learning rate. [...]'
    title: "[arXiv] Stiffness: A New Perspective on Generalization in Neural Networks"
    type: paper
    url: https://arxiv.org/abs/1901.09491
  - category: theory
    description:
    lab: 'MIT EECS, Google Research'
    quote: 'Adaptive methods such as Adam and RMSProp are widely used in deep learning
      but are not well understood. In this paper, we seek a crisp, clean and precise
      characterization of their behavior in nonconvex settings. To this end, we first
      provide a novel view of adaptive methods as preconditioned SGD, where the preconditioner
      is estimated in an online manner. By studying the preconditioner on its own,
      we elucidate its purpose: it rescales the stochastic gradient noise to be isotropic
      near stationary points, which helps escape saddle points. Furthermore, we show
      that adaptive methods can efficiently estimate the aforementioned preconditioner.
      By gluing together these two components, we provide the first (to our knowledge)
      second-order convergence result for any adaptive method. The key insight from
      our analysis is that, compared to SGD, adaptive methods escape saddle points
      faster, and can converge faster overall to second-order stationary points.'
    title: "[arXiv] Escaping Saddle Points with Adaptive Gradient Methods"
    type: paper
    url: https://arxiv.org/abs/1901.09149
  - category: laugh
    description:
    lab: 'Masha Naslidnyk'
    title: 'They diverge later on.â€¦'
    type: twitter
    url: https://twitter.com/NMasha11/status/1089589075567366144
intro_text: ""
outro_text: ""
