intro_text:

outro_text:

articles:
    - title: "An Intuitive Guide to Optimal Transport (Part I and II)"
      url: "https://www.mindcodec.com/an-intuitive-guide-to-optimal-transport-for-machine-learning/"
      type: "blog"
      category: "ml"
      lab: "Luca Ambrogioni"
      quote: ""
      description: "The whole field of machine learning is exploring more and more the properties of optimal transport as an alternative to Kullback-Leibler divergence. This post is a short but clear and interesting introduction to optimal transport. The author offers different views (from intuition to discrete formulation and continuous formulation). The <a href=https://www.mindcodec.com/an-intuitive-guide-to-optimal-transport-part-ii-the-wasserstein-gan-made-easy/>second part</a> deals more specifically with a highlight of last year: the <a href=https://arxiv.org/abs/1701.07875>Wasserstein GAN</a>. On the same subject, you can also find a recent application of optimal transport to NLP in the paper <a href=https://arxiv.org/abs/1808.09663>Wasserstein is all you need</a>. To go even further, check out the  workshop presented at NIPS 2017, <a href=https://vimeo.com/248504509>A Primer on Optimal Transport</a> by Cuturi and Solomon. "
      recommended: ""

    - title: "Google Dataset Search"
      url: "https://toolbox.google.com/datasetsearch"
      type: ""
      category: ""
      lab: "Google"
      description: "The title says it all... Use the power of Google search to find datasets!"
      quote: ""
      recommended: ""

    - title: "TensorFlow 2.0 Changes"
      url: "https://www.youtube.com/watch?v=WTNH0tcscqo"
      type: "video"
      category: "engineering"
      lab: "Aurelien Geron"
      description: "Tensorflow 2.0 is coming fast and there are a number of reasons to be excited. This video gives an overview of the major changes brought by the update. If you are a Pytorch user, you will like the multiple comparisons that are being made between TF and PT. It appears that the difference is going to be tighter and tighter as Tensorflow will now use its Eager mode as default. We will all also appreciate the cleanup of the <i>contrib</i> part of the project."
      quote: ""
      recommended: ""

    - title: "Simple diagrams of convoluted neural networks"
      url: "https://medium.com/inbrowserai/simple-diagrams-of-convoluted-neural-networks-39c097d2925b"
      type: "medium"
      category: ""
      lab: "Piotr Migdał"
      description: "One can be surprised to notice that there is still no common, unified way of graphically representing neural networks in papers. We could argue that the diversity of architectures can't be represented though a unique type of visualisation. While this blog post does not offer an explicit solution, its author does a great job at going over different types of visualisations that have been tested in the past, while highlighting their respective strenghts and weaknesses. I particularly liked the end of the post where you will find a list of tools that can help you build a good visualisation of your architecture. "
      quote: ""
      recommended: ""

    - title: "ICM 2018 - Dynamical, symplectic and stochastic perspectives on optimization"
      url: "https://www.youtube.com/watch?v=wXNWVhE2Dl4"
      type: "video"
      category: "optim"
      lab: "Michael Jordan"
      description: "A 1 hour-long presentation given at the International Congress of Mathematians that was held in early August this year."
      quote: "Our topic is the relationship between dynamical systems and optimization. [...] We aim to build some new connections in this general area, studying aspects of gradient-based optimization from a continuous-time, variational point of view. We go beyond classical gradient flow to focus on second-order dynamics, aiming to show the relevance of such dynamics to optimization algorithms that not only converge, but converge quickly."
      recommended: ""

    - title: "GINN: Geometric Illustrations for Neural Networks"
      url: "http://www.bayeswatch.com/2018/09/17/GINN/"
      type: "blog"
      category: "ml"
      lab: "Luke Darlow"
      description: "This blog post offers a rather entertaining demo that you can find <a href=http://www.bayeswatch.com/assets/ginn/good3.html>here</a>. A shallow fully-connected ReLU-activated network is trained to output a fixed predefined shape. The demo lets you see how the decision frontier of the different ReLU units evolve over time. As the post indicates, we can observe how it is the biases which are mainly modified in the first iterations and how deeper layers build a more comprehensive representation of the image."
      quote: ""
      recommended: ""

    - title: "Hamiltonian Descent Methods"
      url: "https://arxiv.org/abs/1809.05042"
      type: "paper"
      category: "optim"
      lab: "University of Oxford, DeepMind"
      description: ""
      quote: "We propose a family of optimization methods that achieve linear convergence using first-order gradient information and constant step sizes on a class of convex functions much larger than the smooth and strongly convex ones. [...] They are first-order in the sense that they require only gradient computation. [...] In sum, these methods expand the class of convex functions on which linear convergence is possible with first-order computation."
      recommended: ""

    - title: "Neural Processes in PyTorch"
      url: "https://chrisorm.github.io/NGP.html"
      type: "blog"
      category: "ml"
      lab: "Chris Ormandy"
      description: "A few weeks ago, we shared a <a href=https://kasparmartens.rbind.io/post/np/>blog post</a> giving an introduction to one of DeepMind's latest work: neural processes. The goal of this technique is to model a distribution of <i>functions</i> (rather than a distribution of vectors) and to use neural networks to do so. In other words, it can be seen as the 'deep learning' equivalent to gaussian processes. This blog post offers a more hands-on view as code is made available to create neural processes. "
      quote: ""
      recommended: ""

    - title: "Counterfactual Regret Minimization – the core of Poker AI beating professional players"
      url: "https://int8.io/counterfactual-regret-minimization-for-poker-ai/"
      type: "blog"
      category: "rl"
      lab: "Kamil Czarnogórski"
      description: "In recent years, artificial intelligence was able to beat professional poker players in Heads Up Texas Hold'em. This post goes over the progress that has been made over the last few years and offers a detailed explanation of counterfactual regret minimization: the core algorithm that won those games. Dedicate a bit of free time as the post is lengthy (in the good sense!)."
      quote: ""
      recommended: ""

    - title: "The submitted papers at ICLR 2019 are out for reviews"
      url: "https://openreview.net/group?id=ICLR.cc/2019/Conference"
      type: "paper"
      category: "dl"
      lab: "ICLR"
      description: "The submission deadline was yesterday. Now it's time for reviewers to go over the papers. The OpenReview system lets us read all the submissions still!"
      quote: ""
      recommended: ""

    - title: "Fast Computation of Uncertainty in Deep Learning"
      url: "https://emtiyaz.github.io/papers/ki_riken_2018.pdf"
      type: "paper"
      category: "optim"
      lab: "Mohammad Emtiyaz Khan"
      description: "These are the slides given at a presentation in Singapore. They can serve as an introduction to the paper the author published at ICLM 2018 with his colleagues (see <a href=https://arxiv.org/abs/1806.04854>here</a>). They perturb the gradient updates during optimisation in such a way that performing variational inference is made easier."
      quote: ""
      recommended: ""
