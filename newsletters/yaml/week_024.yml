intro_text:

outro_text:

articles:
    - title: "Differentiable plasticity: training plastic neural networks with backpropagation"
      url: "https://arxiv.org/abs/1804.02464"
      type: "paper"
      category: "ml"
      lab: "Uber AI"
      description: "11 weeks ago, we were already talking about this work, pointing to Uber's blogpost. Now they formally published their paper, which has been accepted to ICML 2018."
      quote: "We show that plasticity, just like connection weights, can be optimized by gradient descent in large (millions of parameters) recurrent networks with Hebbian plastic connections. First, recurrent plastic networks with more than two million parameters can be trained to memorize and reconstruct sets of novel, high-dimensional 1000+ pixels natural images not seen during training."
      recommended: ""

    - title: "Moviebox"
      url: "https://github.com/klauscfhq/moviebox"
      type: "github"
      category: ""
      lab: "Klaus Sinani"
      description: "A CLI movie-recommender system that can installed with pip :-)... If you don't know what to watch this week-end..."
      quote: ""
      recommended: ""

    - title: "NVIDIA DALI"
      url: "https://github.com/NVIDIA/dali"
      type: "github"
      category: "ml"
      lab: "NVIDIA"
      description: "Worth keeping that tool in mind. It supports a multitudr of formats (JPEG,  TFRecords, RecordIO,...)."
      quote: "NVIDIA Data Loading Library (DALI) is a collection of highly optimized building blocks and an execution engine to accelerate input data pre-processing for deep learning applications. DALI provides both performance and flexibility of accelerating different data pipelines, as a single library, that can be easily integrated into different deep learning training and inference applications."
      recommended: ""

    - title: "DropBack: Continuous Pruning During Training"
      url: "https://arxiv.org/abs/1806.06949"
      type: "paper"
      category: "ml"
      lab: "The University of British Columbia"
      description: ""
      quote: "We introduce a technique that compresses deep neural networks both during and after training by constraining the total number of weights updated during backpropagation to those with the highest total gradients. The remaining weights are forgotten and their initial value is regenerated at every access to avoid storing them in memory. This dramatically reduces the number of off-chip memory accesses during both training and inference, a key component of the energy needs of DNN accelerators. "
      recommended: ""

    - title: "DensePose"
      url: "https://github.com/facebookresearch/DensePose"
      type: "github"
      category: "cv"
      lab: "Facebook Research"
      description: "The code of DensePose, the paper presented by Zhiyuan, has been released."
      quote: ""
      recommended: ""

    - title: "CVPR 2018 Best Paper Award - Taskonomy: Disentangling Task Transfer Learning"
      url: "https://arxiv.org/abs/1804.08328"
      type: "paper"
      category: "cv"
      lab: "Stanford University, University of California, Berkeley"
      description: ""
      quote: "Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, e.g., to seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity. <br /> 
      We propose a fully computational approach for modeling the structure of space of visual tasks. This is done via finding (first and higher-order) transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks in a latent space. The product is a computational taxonomic map for task transfer learning."
      recommended: ""

    - title: "CVPR 2018 Best Student Paper Award - 
      Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies"
      url: "https://arxiv.org/abs/1801.01615"
      type: "paper"
      category: "cv"
      lab: "Carnegie Mellon University"
      description: ""
      quote: "We present a unified deformation model for the markerless capture of multiple scales of human movement, including facial expressions, body motion, and hand gestures. An initial model is generated by locally stitching together models of the individual parts of the human body, which we refer to as the 'Frankenstein' model. This model enables the full expression of part movements, including face and hands by a single seamless model. "
      recommended: ""

    - title: "For the love of KL"
      url: "https://twitter.com/rvcraiu/status/1009442845160337409"
      type: ""
      category: "laugh"
      lab: ""
      description: ""
      quote: ""
      recommended: ""

    - title: "Neural Ordinary Differential Equations"
      url: "https://arxiv.org/abs/1806.07366"
      type: "paper"
      category: "dl"
      lab: "University of Toronto, Vector Institute"
      description: ""
      quote: "We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. "
      recommended: ""

    - title: "On Deep Tensor Networks and the Nature of Non-Linearity"
      url: "http://outlace.com/TensorNets1.html"
      type: "blog"
      category: "ml"
      lab: "Outlace"
      description: "An introduction to Tensor Networks."
      quote: ""
      recommended: ""

    - title: "ICML 2018 - Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks"
      url: "https://arxiv.org/abs/1806.05393"
      type: "paper"
      category: "theory"
      lab: "Google Brain"
      description: ""
      quote: "In this work, we demonstrate that it is possible to train vanilla CNNs with ten thousand layers or more simply by using an appropriate initialization scheme. We derive this initialization scheme theoretically by developing a mean field theory for signal propagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the input-output Jacobian matrix. These conditions require that the convolution operator be an orthogonal transformation in the sense that it is norm-preserving. We present an algorithm for generating such random initial orthogonal convolution kernels and demonstrate empirically that they enable efficient training of extremely deep architectures."
      recommended: ""

    - title: "Insights on representational similarity in neural networks with canonical correlation"
      url: "https://arxiv.org/abs/1806.05759"
      type: "paper"
      category: "theory"
      lab: "DeepMind, Google Brain"
      description: "AN introductory blogpost <a href=https://ai.googleblog.com/2018/06/how-can-neural-network-similarity-help.html>here</a>."
      quote: "Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method. We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations."
      recommended: ""
