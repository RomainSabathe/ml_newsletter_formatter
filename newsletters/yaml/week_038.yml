intro_text:

outro_text:

articles:
    - title: "An Introduction to Causal Inference with Gaussian Processes (Part I)"
      url: "https://mindcodec.com/an-introduction-to-causal-inference-with-gaussian-processes-part-i/"
      type: blog
      category: ml
      lab: Max Hinne
      description: "This series of articles presents the GP CaKe model (initially published at NIPS 2017). This model can be thought as an alternative to vector autoregression and dynamical systems theory for time-series prediction. In this first edition, we get a refresher of VAR and DST as well as an introductory example to GP CaKe."
      quote: ""
      recommended: 

    - title: "Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups"
      url: "https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255"
      type: medium
      category: engineering
      lab: Thomas Wolf
      description: "Perhaps at nights you have nightmares where the words <i>CUDA error: out of memory</i> haunt you. Fear not, this blog post will give you a few tips to solve your problems. A wide range of solutions is proposed: parallel training, distributing training, gradient-checkpointing,... Note that it is mostly intended to PyTorch users."
      quote: ""
      recommended: 

    - title: "Regression quattro stagioni"
      url: "https://peterroelants.github.io/posts/linear-regression-four-ways/"
      type: blog
      category: ml
      lab: Peter Roelants 
      description: "This post proposes a great refresher on maximum likelihood estimation, maximum a posteriori, ordinary least square, gradient descent and even MCMC parameter estimation using Metropolis-Hastings sampling. The setting is simple: one problem (linear regression) and four ways to formulate the problem and solve it. As always with Peter Roelants, code and plots are provided."
      quote: ""
      recommended: True

    - title: "Real-time in-the-wild robust face detection"
      url: "https://twitter.com/Montreal_AI/status/1054090541397360640"
      type: twitter
      category: laugh
      lab: 
      description: ""
      quote: ""
      recommended: 

    - title: "Rethinking the Value of Network Pruning"
      url: "https://github.com/Eric-mingjie/rethinking-network-pruning"
      type: github + paper
      category: dl
      lab: UC Berkeley, Tsinghua University
      description: "In this work submitted to ICLR 2019, authors explore the actual benefits of pruning a large network. Surprisingly, they find that to achieve a same level of performance, it is not necessary to first train a large model and then prune it. Training from scratch the smaller architecture is enough to reach the same performance. Authors provide code for their experiments."
      quote: "[...] Our results have several implications: 1) training a large, over-parameterized model is not necessary to obtain an efficient final model, 2) learned 'important' weights of the large model are not necessarily useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited 'important' weights, is what leads to the efficiency benefit in the final model, which suggests that some pruning algorithms could be seen as performing network architecture search."
      recommended: 

    - title: "Faster R-CNN and Mask R-CNN in PyTorch 1.0"
      url: "https://github.com/facebookresearch/maskrcnn-benchmark"
      type: github
      category: cv
      lab: Facebook Research
      description: "Remember the <a href=https://github.com/facebookresearch/Detectron>Detectron</a> repo? It was actually featuring in the third issue of this newsletter, back in January 2018! Well time flies and so does technology, and software. Taking advantage of PyTorch 1.0, this repo offers implementations and weights of RPN, Faster R-CNN and Mask R-CNN that match or exceed Detectron in terms of accuracy. But two times faster. With training code, support for batched inference, multi-GPU training and inference,... A great resource once again!"
      quote: ""
      recommended: 

    - title: "Do Deep Generative Models Know What They Don't Know?"
      url: "https://arxiv.org/abs/1810.09136"
      type: paper
      category: theory
      lab: DeepMind
      description: "I came across this paper while reading <a href=https://openreview.net/forum?id=HyxCxhRcY7>Deep Anomaly Detection with Outlier Exposure</a> where a density-estimation model was trained on CIFAR-10. Yet it assigned higher densities to images from SVHN compared to images from CIFAR-10! This submission to ICLR 2019 precisely explores this phenomena. Considering that most cutting-edge out-of-distribution classifiers are based on a similar density-estimation framework, there is no doubt this work will receive some interest."
      quote: "Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the model density from flow-based models, VAEs and PixelCNN cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former."
      recommended: 

    - title: "Learned optimizers that outperform SGD on wall-clock and validation loss"
      url: "https://arxiv.org/abs/1810.10180"
      type: paper
      category: theory
      lab: Google Brain
      description: ""
      quote: "[...] Learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice. [...] In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance. This allows us to train neural networks to perform optimization of a specific task faster than well tuned first-order methods. [...]"
      recommended: 

    - title: "Partial Convolutions for Image Inpainting using Keras"
      url: "https://github.com/MathiasGruber/PConv-Keras"
      type: github
      category: cv
      lab: Mathias Gruber
      description: "Back in April, Nvidia was publishing <a href=http://openaccess.thecvf.com/content_ECCV_2018/papers/Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper.pdf>Image Inpainting for Irregular Holes Using
      Partial Convolutions</a> to ECCV. Needless to say the results were astonishing. Mr Gruber decided to implement the method and to open source his code. His results are just as breath-taking as the ones in the paper!"
      quote: ""
      recommended: 

    - title: "Modular Active Learning framework for Python3"
      url: "https://github.com/modAL-python/modAL"
      type: github
      category: ml
      lab: Tivadar Danka
      description: "Note that the Github page offers nice explanations and visuals."
      quote: "modAL is an active learning framework for Python3, designed with modularity, flexibility and extensibility in mind. Built on top of scikit-learn, it allows you to rapidly create active learning workflows with nearly complete freedom. What is more, you can easily replace parts with your custom built solutions, allowing you to design novel algorithms with ease."
      recommended: 
