intro_text:

outro_text:

articles:
    - title: "Moving Beyond Translation with the Universal Transformer"
      url: "https://ai.googleblog.com/2018/08/moving-beyond-translation-with.html"
      type: "blog"
      category: "nlp"
      lab: "Google AI"
      description: "Last year, the Transformer paper introduced some new, clever ways to do NLP (and translation in particular) without using explicit recurrent models. Instead, everything was done through attention. As explained in the blog post, although this method was robust on translation tasks, it failed on toy tasks usually handled well by RNNs (like copying a string or counting characters). Authors are now introducing an extension of their original method called the Universal Transformer which alleviates the shortcomings we mentioned. This blog post explains the idea behind the Universal Transformer and serves as an introduction to the corresponding paper."
      quote: ""
      recommended: ""

    - title: "Exploring the Limits of Weakly Supervised Pretraining"
      url: "https://research.fb.com/publications/exploring-the-limits-of-weakly-supervised-pretraining/"
      type: "paper"
      category: "dl"
      lab: "Facebook Research"
      description: "We have all used models pretrained on ImageNet and we took this for granted, as a good starting point. But what if we could use a much, much larger dataset for pretraining? Like 3 billion images from Instagram? That's the purpose of this paper. I haven't been through it completely but it seems there's potential to gain a few percents in accuracy. Sadly, the weights have not been released (as far as I know)! "
      quote: "Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance."
      recommended: ""

    - title: "Learning Invariances using the Marginal Likelihood"
      url: "https://arxiv.org/abs/1808.05563"
      type: "paper"
      category: "theory"
      lab: "PROWLER.io, University of Cambridge"
      description: "The standard approach in deep learning to make our models robust to deformations is to use data augmentation. This paper explores the possibility of explicitely adding invariance constraints to the model. For this, authors optimize the <i>marginal likelihood</i> instead of the usual likelihood."
      quote: "We argue that invariances should instead be incorporated in the model structure, and learned using the marginal likelihood, which correctly rewards the reduced complexity of invariant models. We demonstrate this for Gaussian process models, due to the ease with which their marginal likelihood can be estimated. Our main contribution is a variational inference scheme for Gaussian processes containing invariances described by a sampling procedure. We learn the sampling procedure by back-propagating through it to maximise the marginal likelihood."
      recommended: ""

    - title: "vid2vid"
      url: "https://github.com/NVIDIA/vid2vid"
      type: "github+paper+youtube"
      category: "dl"
      lab: "NVIDIA, MIT"
      description: "Stop everything and watch <a href=https://www.youtube.com/watch?v=GrP_aOSXt5U&feature=youtu.be>this video</a>. pix2pix HD, but for videos. After having fell off your chair (utch), you can check out the paper and code."
      quote: "Pytorch implementation for high-resolution (e.g., 2048x1024) photorealistic video-to-video translation. It can be used for turning semantic label maps into photo-realistic videos, synthesizing people talking from edge maps, or generating human motions from poses."
      recommended: "True"

    - title: "Exploring Adversarial Reprogramming"
      url: "https://rajatvd.github.io/Exploring-Adversarial-Reprogramming/"
      type: "blog"
      category: "gan"
      lab: "Rajat Vadiraj Dwaraknath"
      description: "An introduction to the <a href=https://arxiv.org/abs/1806.11146>Adversarial Reprogramming of Neural Networks paper</a> submitted recently. In the paper, authors manage to adversarially <i>reprogram</i> a network by making it perform tasks it hasn't been trained on (a given example is: from a network trained to classify ImagetNet images, reprogram it to count squares in an adversarial image). The author of the blog post reproduces the results indicated in the paper and extend it by trying a few ideas of his own. For instance: changing how the adversarial image is encoded, changing the regularization scheme etc."
      quote: ""
      recommended: ""

    - title: "The QuAC dataset"
      url: "https://twitter.com/mark_riedl/status/1032093321559252994"
      type: ""
      category: ""
      lab: ""
      description: "The dataset is real though! See <a href=https://arxiv.org/abs/1808.07036>here</a>."
      quote: ""
      recommended: ""

    - title: "Deep Reinforcement Learning Playground"
      url: "https://github.com/apockill/DRLPlayground/"
      type: "github"
      category: "rl"
      lab: "Alex Thiel"
      description: "An interface between Unity and Python that allows to run experiment in 3d Unity-generated environments. An example can be seen <a href=https://www.youtube.com/watch?v=GDh9NqJTYKA>here</a>."
      quote: ""
      recommended: ""
