articles:
  - category: theory
    description: 'Where does the Xavier and Kaiming initializations come from? Why
      is there a magic number 2? A square root? This blog post is all about this question.
      I found the author particularly mindful of the inexperienced readers as he takes
      the time to explain each term and each result. Another solid although similar
      post on the subject is given by DeepGrid <a href=https://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/>here</a>.'
    lab: 'Pierre Ouannes'
    title: 'How to initialize deep neural networks? Xavier and Kaiming initialization'
    type: blog
    url: https://pouannes.github.io/blog/initialization/
  - category: engineering
    description: 'The experienced practitioner often already knows all the tricks
      presented in this post but it''s always good to see them laid out like so. In
      particular, the author advocates for a systematic approach: start simple first
      (simple architecture, overfit on a single sample), confirm the loss (theoretically
      and numerically on simple cases), check outputs and connections (can the gradient
      flow from output to input?) and diagnose hyperparameters.'
    lab: 'Cecelia Shao'
    title: 'Checklist for debugging neural networks'
    type: blog
    url: https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21
  - category: ml
    description: 'You were looking for a practical introduction to meta-learning and
      to the <a href=https://arxiv.org/abs/1703.03400>MAML</a> type of loss in particular?
      You also wanted to see how to train try a simple network with <a href=https://github.com/google/jax>JAX</a>?
      Eric Jang does both in this introductory blog post. JAX is introduced with toy
      examples and then used to implement MAML with just a few functions. The functional
      approach to training a neural net is not necessarily straightforward for those
      who are used to Tensorflow or Pytorch, so expect to read this blog at least
      a couple times!'
    lab: 'Eric Jang'
    title: 'Meta-Learning in 50 Lines of JAX'
    type: blog
    url: https://blog.evjang.com/2019/02/maml-jax.html?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter
  - category: ml
    description: 'This is a nice blog post where the author introduces the unweighted
      cosine and weighted cosine methods (presented in this <a href=https://openreview.net/forum?id=r1gl7hC5Km>paper</a>
      rejected from ICLR 2019). The two methods can be used when a network is optimised
      for both a primary task and an auxiliary task. The idea is to transform the
      gradient obtained from the auxiliary task in such a way that it doesn''t hamper
      optimisation on the primary task. In the blog post, the author offers a geometrical
      interpretation of how this can be achieved. He also proposes a variation of
      the unweighted/weighted cosine methods and benchmark them on both a toy task
      and a real task (attractiveness classification on the CelebA dataset).'
    lab: '<a href=https://vivien000.github.io/blog/>Unsupervised Thoughts</a>'
    title: 'Learning through Auxiliary Tasks'
    type: blog
    url: https://vivien000.github.io/blog/journal/learning-though-auxiliary_tasks.html?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter
  - category: dl
    description:
    lab: 'Carnegie Mellon University, Microsoft Research Montreal, MILA, Universite
      de Montreal'
    quote: "[...] Our goal is to understand whether a related phenomenon occurs when\
      \ data does not undergo a clear distributional shift. We define a `forgetting\
      \ event' to have occurred when an individual training example transitions from\
      \ being classified correctly to incorrectly over the course of learning. Across\
      \ several benchmark data sets, we find that: (i) certain examples are forgotten\
      \ with high frequency, and some not at all; (ii) a data set's (un)forgettable\
      \ examples generalize across neural architectures; and (iii) based on forgetting\
      \ dynamics, a significant fraction of examples can be omitted from the training\
      \ data set while still maintaining state-of-the-art generalization performance."
    title: "[ICLR 2019] An Empirical Study of Example Forgetting during Deep Neural\
      \ Network Learning"
    type: paper
    url: https://arxiv.org/abs/1812.05159?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter
  - category: dl
    description:
    lab: 'Tsinghua University, Microsoft Research'
    quote: 'Although variational autoencoders (VAEs) represent a widely influential
      deep generative model, many aspects of the underlying energy function remain
      poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder
      assumptions reduce the effectiveness of VAEs in generating realistic samples.
      In this regard, we rigorously analyze the VAE objective, differentiating situations
      where this belief is and is not actually true. We then leverage the corresponding
      insights to develop a simple VAE enhancement that requires no additional hyperparameters
      or sensitive tuning. Quantitatively, this proposal produces crisp samples and
      stable FID scores that are actually competitive with a variety of GAN models,
      all while retaining desirable attributes of the original VAE architecture. [...]'
    title: "[ICLR 2019] Diagnosing and Enhancing VAE Models"
    type: paper
    url: https://arxiv.org/abs/1903.05789v1
  - category: engineering
    description:
    lab: 'Fujitsu Laboratories'
    quote: "[...]. Distributed deep learning using the large mini-batch is a key technology\
      \ to address the demand and is a great challenge as it is difficult to achieve\
      \ high scalability on large clusters without compromising accuracy. In this\
      \ paper, we introduce optimization methods which we applied to this challenge.\
      \ We achieved the training time of 74.7 seconds using 2,048 GPUs on ABCI cluster\
      \ applying these methods. The training throughput is over 1.73 million images/sec\
      \ and the top-1 validation accuracy is 75.08%."
    title: "[arXiv] Yet Another Accelerated SGD: ResNet-50 Training on ImageNet in\
      \ 74.7 seconds"
    type: paper
    url: https://arxiv.org/abs/1903.12650
  - category: theory
    description:
    lab: 'University of California, University of Southern California'
    quote: "[...] we show that first order methods such as gradient descent are provably\
      \ robust to noise/corruption on a constant fraction of the labels despite over-parametrization\
      \ under a rich dataset model. In particular: i) First, we show that in the first\
      \ few iterations where the updates are still in the vicinity of the initialization\
      \ these algorithms only fit to the correct labels essentially ignoring the noisy\
      \ labels. ii) Secondly, we prove that to start to overfit to the noisy labels\
      \ these algorithms must stray rather far from from the initial model which can\
      \ only occur after many more iterations. [...]"
    title: "[arXiv] Gradient Descent with Early Stopping is Provably Robust to Label\
      \ Noise for Overparameterized Neural Networks"
    type: paper
    url: https://arxiv.org/abs/1903.11680
  - category: dl
    description:
    lab: 'University of Oxford'
    quote: 'We present a novel clustering objective that learns a neural network classifier
      from scratch, given only unlabelled data samples. The model discovers clusters
      that accurately match semantic classes, achieving state-of-the-art results in
      eight unsupervised clustering benchmarks spanning image classification and segmentation.
      [...] The trained network directly outputs semantic labels, rather than high
      dimensional representations that need external processing to be usable for semantic
      clustering. The objective is simply to maximise mutual information between the
      class assignments of each pair. [...]'
    title: "[arXiv] Invariant Information Clustering for Unsupervised Image Classification\
      \ and Segmentation"
    type: paper
    url: https://arxiv.org/abs/1807.06653
intro_text: ""
outro_text: ""
