articles:
  - category: nlp
    description: 'We talked about BERT a few times; a work that had a great impact
      in the NLP community. In this post you will find a summary of where BERT comes
      from and some of the ground laying works that have preceded it, like ELMo and
      the Transformer.'
    lab: 'Lilian Weng'
    quote: "[....] We will discuss the models on learning contextualized word vectors,\
      \ as well as the new trend in large unsupervised pre-trained language models\
      \ which have achieved amazing SOTA results on a variety of language tasks."
    title: 'Generalized Language Models'
    type: blog
    url: https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html
  - category: dl
    description:
    lab: 'MIT, Google Brain, Stanford University'
    quote: 'Normalization layers are a staple in state-of-the-art deep neural network
      architectures. They are widely believed to stabilize training, enable higher
      learning rate, accelerate convergence and improve generalization, [...]. In
      this work, we challenge the commonly-held beliefs by showing that none of the
      perceived benefits is unique to normalization. Specifically, we propose fixed-update
      initialization (Fixup), [...]. We find training residual networks with Fixup
      to be as stable as training with normalization -- even for networks with 10,000
      layers. Furthermore, with proper regularization, Fixup enables residual networks
      without normalization to achieve state-of-the-art performance in image classification
      and machine translation'
    title: "[ICLR 19] Fixup Initialization: Residual Learning Without Normalization"
    type: paper
    url: https://openreview.net/forum?id=H1gsz30cKX
  - category: dl
    description:
    lab: 'UC Berkeley, covariant.ai'
    quote: 'Flow-based generative models are powerful exact likelihood models with
      efficient sampling and inference. Despite their computational efficiency, flow-based
      models generally have much worse density modeling performance compared to state-of-the-art
      autoregressive models. In this paper, we investigate and improve upon three
      limiting design choices employed by flow-based models in prior work [..]. Based
      on our findings, we propose Flow++, a new flow-based model that is now the state-of-the-art
      non-autoregressive model for unconditional density estimation on standard image
      benchmarks. [...]'
    title: "[arXiv] Flow++: Improving Flow-Based Generative Models with Variational\
      \ Dequantization and Architecture Design"
    type: paper
    url: https://arxiv.org/abs/1902.00275
  - category: theory
    description:
    lab: 'Mila, Universite de Montreal'
    quote: 'We revisit the bias-variance tradeoff for neural networks in light of
      modern empirical findings. The traditional bias-variance tradeoff in machine
      learning suggests that as model capacity grows, bias decreases and variance
      increases. In contrast, we find that both bias and variance can decrease with
      the number of parameters. To better understand this, we decompose the total
      variance into variance due to training set sampling and variance due to initialization.
      Surprisingly, variance due to training set sampling is roughly constant with
      both network width and depth, in the over-parameterized setting. We provide
      theoretical analysis, in a simplified setting inspired by linear models, that
      is consistent with our empirical findings.'
    title: "[arXiv] A Modern Take on the Bias-Variance Tradeoff in Neural Networks"
    type: paper
    url: https://arxiv.org/abs/1810.08591
  - category: theory
    description:
    lab: Google
    quote: "[...] We study empirically the layer-wise functional structure of over-parameterized\
      \ deep models. We provide evidence for the heterogeneous characteristic of layers.\
      \ To do so, we introduce the notion of (post training) re-initialization and\
      \ re-randomization robustness. We show that layers can be categorized into either\
      \ 'robust' or 'critical'. In contrast to critical layers, resetting the robust\
      \ layers to their initial value has no negative consequence, and in many cases\
      \ they barely change throughout training. Our study provides further evidence\
      \ that mere parameter counting or norm accounting is too coarse in studying\
      \ generalization of deep models."
    title: "[arXiv] Are All Layers Created Equal?"
    type: paper
    url: https://arxiv.org/abs/1902.01996
  - category: dl
    description: 'A few weeks ago, we talked about <a href=https://arxiv.org/abs/1812.04948>StyleGAN</a>,
      a new GAN architecture that is capable of generating high-resolution realistic
      images. In addition, the architecture incorporates possibilities to tweak the
      <i>content</i> of an image relative to its <i>style</i>. Authors just released
      their implementation and people on the internet have already started playing
      around with it. Some mandatory cats can be found <a href=https://twitter.com/genekogan/status/1093180351437029376>here</a>
      while creepy failure cases can be found <a href=https://twitter.com/kcimc/status/1092827997978181632>here</a>.
      Happy hacking!'
    lab: NVIDIA
    title: 'StyleGAN - Official TensorFlow Implementation'
    type: github
    url: https://github.com/NVlabs/stylegan
  - category: theory
    description: 'This is a small but instructive post on variational autoencoders.
      The author had one objective in mind: remind us that variational autoencoders
      are not meant to be autoencoders. The ''autoencoding'' aspect of those models
      is just a trick to obtain a lower bound on the log-likelihood on the data distribution
      generated by the models. For this we introduce an approximate posterior that
      is obtained thanks to what we usually call an <i>encoder</i>. But as the author
      shows, an optimal solution to the problem is obtained once we don''t need the
      encoder anymore; i.e. when the generator is independent from the latent variable.
      In other words: an optimal ''variational autoencoder'' is not an autoencoder
      as it doesn''t use the latent variable (and therefore its input).'
    lab: 'Paul Rubenstein'
    title: 'Variational Autoencoders are not autoencoders'
    type: blog
    url: http://paulrubenstein.co.uk/variational-autoencoders-are-not-autoencoders/
  - category: laugh
    description:
    lab: 'Ankur Handa'
    title: 'YOLOv3 and the art of the Introduction'
    type: paper
    url: https://twitter.com/ankurhandos/status/1091531295354580992
intro_text: ""
outro_text: ""
