intro_text:

outro_text:

articles:
    - title: "Metro"
      recommended: False
      type: ""
      category: "Everything Else"
      lab: ""
      url: "https://metro.exchange/"
      description: "A platform created by two undergraduate students and which aim at providing a platform to sell and buy data."
      quote:
          "We empower the community to control their data and we help innovators achieve their goals."


    - title: "word2vec graph"
      recommended: False
      type: "github"
      category: "Natural Language Processing"
      lab: "anvaka"
      url: "https://github.com/anvaka/word2vec-graph"
      description: "A visualisation tool to inspect high-dimensional word2vec embeddings. The link provides GIFs to show you what to expect. It looks pretty cool. :-)"
      quote:
          ""


    - title: "Real-time Semantic Segmentation Comparative Study"
      recommended: False
      type: "github+paper"
      category: "Computer Vision"
      lab: "Mennatullah Siam, Mostafa Gamal, Moemen AbdelRazek, Senthil Yogamani, Martin Jagersand"
      url: "https://github.com/MSiam/TFSegmentation"
      description: "The source code used in the paper <a href=https://arxiv.org/abs/1803.02758>RTSeg: Real-time Semantic Segmentation Comparative Study</a>. They compared different feature-extraction architectures (VGG, ResNet, MobileNet, ShuffleNet) coupled with different decoding methods (UNet, SkipNet, Dilation...). The setting is essentially for a driving car; so road-like scenery segmentation. Oh, and the goal is to achieve real-time segmentation so MobileNet seems to obtain good scores."
      quote:
          ""


    - title: "HALP: High-Accuracy Low-Precision Training"
      recommended: False
      type: "blog"
      category: "General Machine Learning"
      lab: "Stanford Dawn - Chris De Sa, Megan Leszczynski, Jian Zhang, Alana Marzoev, Chris Aberger, Kunle Olukotun, and Chris Ré"
      url: "http://dawn.cs.stanford.edu/2018/03/09/low-precision/"
      description: "To get a good training, we need to use high precision numbers (encoded on at least 32 bits and more broadly on 64 bits). This blog post describes an idea which enables to use low precision numbers (on 8 bits for instance). The idea is to refine the range described by those 8 bits during training. During early training, the gradients will have a high value so the 8 bits must cover a high range of values (say from -5 to 5). But as training goes on, gradients become smaller and we can use those 8 bits to cover a narrower range of values (say from -0.1 to 0.1); hence preserving a decent amount of precision. The algorithm has theoretical grounds for strongly convex problems. "
      quote:
          "We describe a new variant of stochastic gradient descent (SGD) called high-accuracy low precision (HALP) that can sometimes get high-accuracy solutions from low-precision training."


    - title: "How I implemented iPhone X’s FaceID using Deep Learning in Python"
      recommended: False
      type: "medium"
      category: "Computer Vision"
      lab: "University of Rome - Norman Di Palo"
      url: "https://towardsdatascience.com/how-i-implemented-iphone-xs-faceid-using-deep-learning-in-python-d5dbaa128e1d"
      description: "Di Palo implements a siamese network with an RGB-D camera and trains it on a publicly available dataset."
      quote:
          ""

    - title: "Train Your Machine Learning Models on Google’s GPUs for Free — Forever"
      recommended: False
      type: "medium"
      category: "News"
      lab: "Mick Bourdakos"
      url: "https://hackernoon.com/train-your-machine-learning-models-on-googles-gpus-for-free-forever-a41bd309d6ad"
      description: "Sounds like clickbait but apparently Google gives access to a Jupyter-like interface and you use a GPU up to 12hrs straight. After that you get disconnected. Oh, and the GPU memory is shared so likely you'll only be able to fit small models. I wonder what the EULA looks like..."
      quote:
          ""

    - title: "Common Patterns for Analyzing Data"
      recommended: False
      type: "blog"
      category: "Data Science & Visualisations"
      lab: "Kevin Scott"
      url: "https://thekevinscott.com/common-patterns-for-analyzing-data/"
      description: "An intro (but detailed) on data analysis and feature engineering."
      quote:
          ""

    - title: "Distributed Deep Learning, Part 1: An Introduction to Distributed Training of Neural Networks"
      recommended: False
      type: "blog"
      category: "Production & Engineering"
      lab: "Skymind"
      url: "https://blog.skymind.ai/distributed-deep-learning-part-1-an-introduction-to-distributed-training-of-neural-networks/"
      description: "Great intro on distributed computing for machine learning."
      quote:
          ""


    - title: "Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN"
      recommended: False
      type: "paper"
      category: "Deep Learning"
      lab: "CVPR 2018 - University of Wollongong"
      url: "https://arxiv.org/abs/1803.04831"
      description: "Edit March 15th: this field is crazy... Anyways. The paper has been uploaded on Arvix two days ago and you can already find implementations on Github. Here is one with <a href=https://github.com/batzner/indrnn>Tensorflow</a> and another one with <a href=https://github.com/theSage21/Indrnn>Pytorch</a>."
      quote:
          "[We propose] a new type of RNN, referred to as independently recurrent neural network (IndRNN), neurons in the same layer are independent of each other and they are connected across layers. We show that an IndRNN can be easily regulated to prevent the gradient exploding and vanishing problems while allowing the network to learn long-term dependencies. "


    - title: "Transfer Your Font Style with GANs"
      recommended: False
      type: "blog+paper"
      category: "GANs & Adversarial Attacks"
      lab: "BAIR - Samaned Azadi"
      url: "http://bair.berkeley.edu/blog/2018/03/13/mcgan/"
      description: ""
      quote:
          "Instead of training a single network for all possible typeface ornamentations, we designed the multi-content GAN architecture [2] to retrain a customized magical network for each observed character set with only a handful of observed glyphs. [...] The multi-content GAN model consists of a stacked cGAN architecture to predict the coarse glyph shapes and an ornamentation network to predict color and texture of the final glyphs. "


    - title: "Open NSynth Super"
      recommended: False
      type: "github"
      category: "Everything Else"
      lab: "Google Magenta"
      url: "https://github.com/googlecreativelab/open-nsynth-super"
      description: "Magenta has released... a device! It is open source (that's awesome). It's a tactile board with 4 corners and an audio output. By moving your finger on the board, the device will produce a sound as a combination of 4 instruments (you can select those instruments). The closer your finger from a corner, the more obvious it will sound from a specific instrument. From a small chat I had at NIPS, it seems the underlying algorithm is based on a Variational Autoencoder. By defining 4 instruments, you can define an hyperplane in the embedding space and move along it. I imagine a tricky part is to produce a non-noisy sound from a given embedding...<br/>Edit Friday: here is a <a href=https://magenta.tensorflow.org/music-vae>blog post</a> explaining the theory behind it. VAE indeed!"
      quote:
          ""

    - title: "Averaging Weights Leads to Wider Optima and Better Generalization"
      recommended: False
      type: "paper"
      category: "Optimisation & Learning Theory"
      lab: "Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson"
      url: "https://arxiv.org/abs/1803.05407"
      description: ""
      quote:
          "We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model."


    - title: "Microsoft uses AI to match human performance in translating news from Chinese to English"
      recommended: False
      type: "blog"
      category: "News"
      lab: "Microsoft"
      url: "https://blogs.microsoft.com/ai/machine-translation-news-test-set-human-parity/?wt.mc_id=74788-mcr-fb"
      description: ""
      quote:
          ""

          
    - title: "PyTorch – Internal Architecture Tour"
      recommended: False
      type: "blog"
      category: "Production & Engineering"
      lab: "Christian Perone"
      url: "http://blog.christianperone.com/2018/03/pytorch-internal-architecture-tour/"
      description: "Better be comfortable with C to really enjoy this blog post. It is really neat nonetheless. The authors shows the internals of Pytorch (as it binds C and Python, how objects are stored etc.)"
      quote:
          ""

    - title: "Variance Networks: When Expectation Does Not Meet Your Expectations"
      recommended: False
      type: "paper"
      category: "Deep Learning"
      lab: "Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, Dmitry Vetrov"
      url: "https://arxiv.org/abs/1803.03764"
      description: ""
      quote:
          "In this paper, we propose variance networks, a new model that stores the learned information in the variances of the network weights. Surprisingly, no information gets stored in the expectations of the weights, therefore if we replace these weights with their expectations, we would obtain a random guess quality prediction. We provide a numerical criterion that uses the loss curvature to determine which random variables can be replaced with their expected values, and find that only a small fraction of weights is needed for ensembling."


    - title: "The rosetta stone of Deep Learning"
      recommended: True
      type: "github"
      category: "Deep Learning"
      lab: "Microsoft Data Science - Ilia Karmanov"
      url: "https://github.com/ilkarman/DeepLearningFrameworks"
      description: "Nice repo if you're familiar with a specific DL framework and want some change. It's basically a collection of notebook. Each of them solves a specific problem (inference in a graph, training a CNN, training an RNN). And there's one notebook per deep learning framework! Allowing you to compare directly the ammount of code required to get a training running. The proposed frameworks are: CNTK, Caffe, Chainer, Gluon, Keras, Knet, MXNet, Pytorch, Tensorflow and Theano.<br/>
      The best part? The author provides benchmark on running times!! Annndddd Pytorch appears to be faster to train compared to Tensorflow, even on multi GPU configuration! However, Tensorflow is about 15% faster for inference on K80s."
      quote:
          ""


    - title: "Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning"
      recommended: false
      type: "github+paper"
      category: "Interpratibility & Fairness"
      lab: "MIT Lincoln Lab, Planck Aerosystems - David Mascharka, Philip Tran, Ryan Soklaski, Arjun Majumdar"
      url: "https://arxiv.org/abs/1803.05268v1"
      description: "Implementation by the authors <a href=https://github.com/davidmascharka/tbd-nets>here</a> (Pytorch). It includes notebooks!! The repo looks super clean with lots of resources."
      quote:
          "We propose a set of visual-reasoning primitives which, when composed, manifest as a model capable of performing complex reasoning tasks in an explicitly-interpretable manner. The fidelity and interpretability of the primitives' outputs enable an unparalleled ability to diagnose the strengths and weaknesses of the resulting model. "


    - title: "Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches"
      recommended: false
      type: "paper"
      category: "Optimisation & Learning Theory"
      lab: "ICLR - University of Toronto, Google - Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches"
      url: "https://arxiv.org/abs/1803.04386v1"
      description: ""
      quote:
          "Due to the large number of weights, all the examples in a mini-batch typically share the same weight perturbation, thereby limiting the variance reduction effect of large mini-batches. We introduce flipout, an efficient method for decorrelating the gradients within a mini-batch by implicitly sampling pseudo-independent weight perturbations for each example. Empirically, flipout achieves the ideal linear variance reduction for fully connected networks, convolutional networks, and RNNs."


    - title: "Using Evolutionary AutoML to Discover Neural Network Architectures"
      recommended: false
      type: "blog"
      category: "General Machine Learning"
      lab: "Google Research"
      url: "https://research.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html"
      description: "A blog post that presents a recent published paper at ICLR in which an evolutionary algorithm is used to generate good-performing network architectures. It's an evolution of their <a href=https://arxiv.org/abs/1707.07012>first paper</a> published last year (and reviewed during one of our paper reading session!): they switched from RL to genetic algorithms."
      quote:
          ""


    - title: "Style Transfer as Optimal Transport"
      recommended: false
      type: "github"
      category: "Deep Learning"
      lab: "Vince Marron"
      url: "https://github.com/VinceMarron/style_transfer"
      description: "The Wasserstein distance stikes again but this time it is not applied to GANs but rather to style transfer."
      quote:
          ""
